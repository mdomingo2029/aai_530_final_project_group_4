{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KtEx43QTbEJ2",
        "outputId": "659b7c9c-c44d-4296-f172-edf6a8be6b85"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ucimlrepo\n",
            "  Downloading ucimlrepo-0.0.7-py3-none-any.whl.metadata (5.5 kB)\n",
            "Requirement already satisfied: pandas>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from ucimlrepo) (2.2.2)\n",
            "Requirement already satisfied: certifi>=2020.12.5 in /usr/local/lib/python3.12/dist-packages (from ucimlrepo) (2026.1.4)\n",
            "Requirement already satisfied: numpy>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.0.0->ucimlrepo) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.0.0->ucimlrepo) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.0.0->ucimlrepo) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.0.0->ucimlrepo) (2025.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas>=1.0.0->ucimlrepo) (1.17.0)\n",
            "Downloading ucimlrepo-0.0.7-py3-none-any.whl (8.0 kB)\n",
            "Installing collected packages: ucimlrepo\n",
            "Successfully installed ucimlrepo-0.0.7\n"
          ]
        }
      ],
      "source": [
        "!pip install ucimlrepo"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H80cVKV3bvUR",
        "outputId": "82d0232b-3709-4eb5-c5da-9d41fd9844d9"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls \"/content/drive/MyDrive/WESAD.zip\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ni1oarDfbvL9",
        "outputId": "9f569512-7e0e-4626-fff6-0f7f468792e4"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/WESAD.zip\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "66158b95",
        "outputId": "2c99085f-1cb3-434f-ff5c-0970dc3986f8"
      },
      "source": [
        "print('Listing contents of MyDrive:')\n",
        "!ls \"/content/drive/MyDrive\""
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Listing contents of MyDrive:\n",
            "'15).xlsx'\n",
            "'2014 Izamar Cardenas Resume.gdoc'\n",
            "'2014 Tax Return Documents (DOMINGO MICHAEL R).pdf'\n",
            "'2015 Tax Return Documents (DOMINGO MICHAEL R).pdf'\n",
            "'2016_Domingo_Resume (1).pdf'\n",
            " 2016_Domingo_Resume.pdf\n",
            "'2016 State Return (Michael Domingo).pdf'\n",
            " 20200304163904.pdf\n",
            "'20250116 2h2'\n",
            "'20250116 2h2 (1)'\n",
            "'2025 Camo to Careers - LinkedIn Coaches Event.pdf'\n",
            "'30 60 90 day Action Plan.docx.gdoc'\n",
            " 8111-50-26.pdf\n",
            "'Business letter (1).gdoc'\n",
            "'Business letter.gdoc'\n",
            "'Colab Notebooks'\n",
            "'Comprehensive SAM.gdoc'\n",
            "'Copy of 20250116 2h2'\n",
            "'Copy of Group 2 Technical Report.gdoc'\n",
            "'Copy of Michael Domingo - ApplyAll Applications.gsheet'\n",
            "'Copy of RO Copy of Japan Itinerary   Guide.gdoc'\n",
            "'Copy of Veterans Day Minitheme by Slidesgo.gslides'\n",
            "'CrossFit 2016 open'\n",
            "'Diabetes Prediction Using CDC Health Indicators (1).gslides'\n",
            "'Diabetes Prediction Using CDC Health Indicators.gslides'\n",
            " domingo.pdf\n",
            " emailreceipt_20170928R0945961941.pdf\n",
            "'Final_Clothing Classification and Clustering Using Deep Learning.gdoc'\n",
            "'Final Project Paper .gdoc'\n",
            " Final-Project-Presentation-Group-2.gslides\n",
            " FullSizeRender.jpg\n",
            "'Gift Letter.docx'\n",
            "'HBOT Research Information Outline'$'\\n''.gdoc'\n",
            "'Help me create another prompt with service members....gdoc'\n",
            "'Houses (1).dat.gsheet'\n",
            " Houses.dat.gsheet\n",
            " Houses.dat.txt\n",
            " Houses.dat.xlsx\n",
            " IDCARD.pdf\n",
            "'IMG_1305 (Converted).mov'\n",
            "'IMG_1379 (Converted).mov'\n",
            " IMG_5885.mp4\n",
            " IMG_5886.mp4\n",
            " Income.dat.gsheet\n",
            " Income.dat.xls\n",
            " invoice_45524.pdf\n",
            "'Japan Trip.gdoc'\n",
            "'Job Application info.gdoc'\n",
            "'Let’s create another prompt with the service membe....gdoc'\n",
            "'Lord of Combat.mp4'\n",
            "'Meal Plan for Alex, Week 3 & 4.gdoc'\n",
            " Michael_Domingo_2016Resume.pdf\n",
            "'MichaelDomingo_Resume2013 (1).docx'\n",
            " MichaelDomingo_Resume2013.docx\n",
            " MichaelDomingoResume2013.docx\n",
            " MichaelDomingo_Resume2013.docx.gdoc\n",
            " MichaelDomingo_Resume2013.gdoc\n",
            "'Mission Statement - The Rinaldi Project.gdoc'\n",
            "'Music Genre and Composer Classification Using Deep Learning.gdoc'\n",
            " Novius\n",
            " offer_to_rent_20170731061252.pdf\n",
            " PK-26-AllHeroes-BabyShower-SEC.pdf\n",
            " Q1.xls\n",
            "'Recovery Lab Logo.eps'\n",
            " RMA_2013-12-31_19-45-39.pdf\n",
            "'Slide 1.m4a'\n",
            "'Slide 2.m4a'\n",
            "'Slide 3.m4a'\n",
            "'Slide 4.m4a'\n",
            "'Slide 5.m4a'\n",
            "'Slide 6.m4a'\n",
            "'Strength and Condition Program - sport specific to baseball.gdoc'\n",
            "'Students (1).dat.txt'\n",
            " Students.dat.txt\n",
            " SUPERBABY-AllHeroes-BShower-PK-26_-SEC.pdf\n",
            "'The Rinaldi Project'\n",
            "'The Rinaldi Project_Articles of Incorporation.pdf'\n",
            "'Untitled document (1).gdoc'\n",
            "'Untitled document (2).gdoc'\n",
            "'Untitled document (3).gdoc'\n",
            "'Untitled document (4).gdoc'\n",
            "'Untitled document (5).gdoc'\n",
            "'Untitled document.gdoc'\n",
            "'Untitled presentation.gslides'\n",
            "'Untitled spreadsheet (1).gsheet'\n",
            "'Untitled spreadsheet (2).gsheet'\n",
            "'Untitled spreadsheet (3).gsheet'\n",
            "'Untitled spreadsheet.gsheet'\n",
            "'Updated_2016_Domingo_Resume (1).pdf'\n",
            " Updated_2016_Domingo_Resume.pdf\n",
            "'Updated 7-day itinerary in Japan.gdoc'\n",
            " USAF\n",
            " USD\n",
            "'UTA March Airman’s Council – Meeting Summary.gdoc'\n",
            "'UTA March Air Reserve Base Airman’s Council Meeting Agenda.gdoc'\n",
            " WESAD.zip\n",
            "'Xendurance 2016 Wholesale Pricing.pdf'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0afa312a"
      },
      "source": [
        "If your `WESAD.zip` file is inside a subfolder, for example, named `MyProject`, you would then run:\n",
        "\n",
        "```python\n",
        "!ls \"/content/drive/MyDrive/MyProject\"\n",
        "```\n",
        "\n",
        "Once you find the file, use the full path in your code."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "\n",
        "# 1. Mount the drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# 2. Define the exact path (Google Drive root is 'MyDrive')\n",
        "zip_path = '/content/drive/MyDrive/WESAD.zip'\n",
        "\n",
        "# 3. Check if the file exists before unzipping\n",
        "if os.path.exists(zip_path):\n",
        "    print(\"Found it! Unzipping now...\")\n",
        "    !unzip -q \"{zip_path}\" -d \"/content/WESAD_data\"\n",
        "    print(\"Done! Files are now in the 'WESAD_data' folder on the left sidebar.\")\n",
        "else:\n",
        "    print(\"Still can't see it. Make sure the file is in the main 'My Drive' folder, not a subfolder.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yH0pkVivmCPM",
        "outputId": "7165920a-bd97-4711-db14-16049a5d12a7"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Found it! Unzipping now...\n",
            "replace /content/WESAD_data/WESAD/S10/S10.pkl? [y]es, [n]o, [A]ll, [N]one, [r]ename: A\n",
            "Done! Files are now in the 'WESAD_data' folder on the left sidebar.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aa8abf94"
      },
      "source": [
        "# Task\n",
        "Explore the structure of the unzipped WESAD data by listing the contents of the `/content/WESAD_data` directory to identify the main data files."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9a124f70"
      },
      "source": [
        "## Explore Unzipped Data\n",
        "\n",
        "### Subtask:\n",
        "Explore the structure of the unzipped WESAD data by listing the contents of the `/content/WESAD_data` directory to identify the main data files.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a59b22cf"
      },
      "source": [
        "**Reasoning**:\n",
        "To explore the structure of the unzipped WESAD data, I will list the contents of the `/content/WESAD_data` directory using the `!ls -F` command as instructed.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ebd61aa7",
        "outputId": "a63143d3-7fc6-4340-fc6b-6e66c293b948"
      },
      "source": [
        "print('Listing contents of the unzipped WESAD data directory:')\n",
        "!ls -F /content/WESAD_data"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Listing contents of the unzipped WESAD data directory:\n",
            "WESAD/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ba54179c"
      },
      "source": [
        "**Reasoning**:\n",
        "Since the previous `ls` command revealed a subdirectory named `WESAD/`, I will now list the contents of this subdirectory to further explore the data structure and locate the actual data files.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cbb4aa9e",
        "outputId": "8207cee3-effc-4e1c-ad5c-4e451d23f31a"
      },
      "source": [
        "print('Listing contents of the WESAD subdirectory:')\n",
        "!ls -F /content/WESAD_data/WESAD"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Listing contents of the WESAD subdirectory:\n",
            "S10/  S13/  S15/  S17/\tS3/  S5/  S7/  S9/\n",
            "S11/  S14/  S16/  S2/\tS4/  S6/  S8/  wesad_readme.pdf\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d6a5b1e7"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous command revealed subject-specific subdirectories (S2/, S3/, etc.) and a `wesad_readme.pdf` file. To understand the data structure further, I will list the contents of one of these subject directories, specifically `S2/`, to see what kind of data files are stored there.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "635b6015",
        "outputId": "e1dae9e8-a4e8-4dab-9de3-2808112d4562"
      },
      "source": [
        "print('Listing contents of the S2 subdirectory:')\n",
        "!ls -F /content/WESAD_data/WESAD/S2"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Listing contents of the S2 subdirectory:\n",
            "S2_E4_Data.zip\tS2.pkl\tS2_quest.csv  S2_readme.txt  S2_respiban.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e963a801"
      },
      "source": [
        "## Load Data\n",
        "\n",
        "### Subtask:\n",
        "Load the relevant WESAD data files into pandas DataFrames for cleaning.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "94ad475b"
      },
      "source": [
        "**Reasoning**:\n",
        "I will import the pandas library, define the base path to the S2 subject data, and then load the 'S2.pkl' and 'S2_quest.csv' files into two separate pandas DataFrames, `df_s2_data` and `df_s2_quest` respectively, to begin the data loading process as instructed.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228
        },
        "id": "b274afb1",
        "outputId": "f132c00f-2b12-48d9-a2d3-75d26e67e985"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Define the base path to the S2 subject data directory\n",
        "s2_data_path = '/content/WESAD_data/WESAD/S2'\n",
        "\n",
        "# Load the S2.pkl file into a pandas DataFrame\n",
        "df_s2_data = pd.read_pickle(f'{s2_data_path}/S2.pkl')\n",
        "\n",
        "# Load the S2_quest.csv file into a pandas DataFrame\n",
        "df_s2_quest = pd.read_csv(f'{s2_data_path}/S2_quest.csv')\n",
        "\n",
        "print(\"df_s2_data head:\")\n",
        "print(df_s2_data.head())\n",
        "\n",
        "print(\"\\ndf_s2_quest head:\")\n",
        "print(df_s2_quest.head())"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "df_s2_data head:\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'dict' object has no attribute 'head'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-354765590.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"df_s2_data head:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_s2_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\ndf_s2_quest head:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'dict' object has no attribute 'head'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c34221d1"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous code failed because `df_s2_data` was a dictionary, not a pandas DataFrame, and therefore did not have a `.head()` method. The `S2.pkl` file, when loaded, is a dictionary containing various data points. To address the subtask of loading relevant data into DataFrames, I will first load the `.pkl` file into a dictionary, then extract the 'chest' and 'wrist' signal data from it and convert them into separate pandas DataFrames. I will also correctly load the `S2_quest.csv` into a DataFrame.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "2d85dabe",
        "outputId": "6e8e8e59-e5b3-4f20-8314-d4e181d337aa"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Define the base path to the S2 subject data directory\n",
        "s2_data_path = '/content/WESAD_data/WESAD/S2'\n",
        "\n",
        "# Load the S2.pkl file into a dictionary first\n",
        "s2_raw_data_dict = pd.read_pickle(f'{s2_data_path}/S2.pkl')\n",
        "\n",
        "# Extract 'chest' and 'wrist' signals and convert them to DataFrames\n",
        "df_s2_chest_data = pd.DataFrame(s2_raw_data_dict['signal']['chest'])\n",
        "df_s2_wrist_data = pd.DataFrame(s2_raw_data_dict['signal']['wrist'])\n",
        "\n",
        "# Load the S2_quest.csv file into a pandas DataFrame\n",
        "df_s2_quest = pd.read_csv(f'{s2_data_path}/S2_quest.csv')\n",
        "\n",
        "print(\"df_s2_chest_data head:\")\n",
        "print(df_s2_chest_data.head())\n",
        "\n",
        "print(\"\\ndf_s2_wrist_data head:\")\n",
        "print(df_s2_wrist_data.head())\n",
        "\n",
        "print(\"\\ndf_s2_quest head:\")\n",
        "print(df_s2_quest.head())"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Per-column arrays must each be 1-dimensional",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3851946167.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# Extract 'chest' and 'wrist' signals and convert them to DataFrames\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mdf_s2_chest_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms2_raw_data_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'signal'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'chest'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0mdf_s2_wrist_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms2_raw_data_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'signal'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'wrist'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[1;32m    776\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    777\u001b[0m             \u001b[0;31m# GH#38939 de facto copy defaults to False only in non-dict cases\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 778\u001b[0;31m             \u001b[0mmgr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict_to_mgr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtyp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmanager\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    779\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMaskedArray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    780\u001b[0m             \u001b[0;32mfrom\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mma\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmrecords\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/internals/construction.py\u001b[0m in \u001b[0;36mdict_to_mgr\u001b[0;34m(data, index, columns, dtype, typ, copy)\u001b[0m\n\u001b[1;32m    501\u001b[0m             \u001b[0marrays\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"dtype\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    502\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 503\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0marrays_to_mgr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtyp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtyp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconsolidate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    504\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    505\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/internals/construction.py\u001b[0m in \u001b[0;36marrays_to_mgr\u001b[0;34m(arrays, columns, index, dtype, verify_integrity, typ, consolidate)\u001b[0m\n\u001b[1;32m    112\u001b[0m         \u001b[0;31m# figure out the index, if necessary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m             \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_extract_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m             \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mensure_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/internals/construction.py\u001b[0m in \u001b[0;36m_extract_index\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m    662\u001b[0m             \u001b[0mraw_lengths\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    663\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mval\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 664\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Per-column arrays must each be 1-dimensional\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    665\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    666\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mindexes\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mraw_lengths\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Per-column arrays must each be 1-dimensional"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3222e2e5"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous code failed because the 'ACC' (accelerometer) data within the 'chest' and 'wrist' signals of the `S2.pkl` file are multi-dimensional arrays, causing a `ValueError` when `pd.DataFrame` tried to interpret them as single columns. To fix this, I need to explicitly extract the 'ACC' data into separate columns (e.g., 'ACC_x', 'ACC_y', 'ACC_z') and then create DataFrames from all the extracted 1-dimensional signal arrays.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "8035ff91",
        "outputId": "5083ea1b-82c6-4e20-aee7-b4e79f1df397"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Define the base path to the S2 subject data directory\n",
        "s2_data_path = '/content/WESAD_data/WESAD/S2'\n",
        "\n",
        "# Load the S2.pkl file into a dictionary first\n",
        "s2_raw_data_dict = pd.read_pickle(f'{s2_data_path}/S2.pkl')\n",
        "\n",
        "# Process 'chest' signals\n",
        "chest_signals = s2_raw_data_dict['signal']['chest']\n",
        "chest_data = {}\n",
        "for sensor_name, data_array in chest_signals.items():\n",
        "    if sensor_name == 'ACC':\n",
        "        # Accelerometer data is typically 3-axis, so create three columns\n",
        "        chest_data['ACC_x'] = data_array[:, 0]\n",
        "        chest_data['ACC_y'] = data_array[:, 1]\n",
        "        chest_data['ACC_z'] = data_array[:, 2]\n",
        "    else:\n",
        "        # Other sensors are typically 1D\n",
        "        chest_data[sensor_name] = data_array\n",
        "\n",
        "df_s2_chest_data = pd.DataFrame(chest_data)\n",
        "\n",
        "# Process 'wrist' signals\n",
        "wrist_signals = s2_raw_data_dict['signal']['wrist']\n",
        "wrist_data = {}\n",
        "for sensor_name, data_array in wrist_signals.items():\n",
        "    if sensor_name == 'ACC':\n",
        "        # Accelerometer data is typically 3-axis, so create three columns\n",
        "        wrist_data['ACC_x'] = data_array[:, 0]\n",
        "        wrist_data['ACC_y'] = data_array[:, 1]\n",
        "        wrist_data['ACC_z'] = data_array[:, 2]\n",
        "    else:\n",
        "        # Other sensors are typically 1D\n",
        "        wrist_data[sensor_name] = data_array\n",
        "\n",
        "df_s2_wrist_data = pd.DataFrame(wrist_data)\n",
        "\n",
        "# Load the S2_quest.csv file into a pandas DataFrame\n",
        "df_s2_quest = pd.read_csv(f'{s2_data_path}/S2_quest.csv')\n",
        "\n",
        "print(\"df_s2_chest_data head:\")\n",
        "print(df_s2_chest_data.head())\n",
        "\n",
        "print(\"\\ndf_s2_wrist_data head:\")\n",
        "print(df_s2_wrist_data.head())\n",
        "\n",
        "print(\"\\ndf_s2_quest head:\")\n",
        "print(df_s2_quest.head())"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Per-column arrays must each be 1-dimensional",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1502836889.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mchest_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msensor_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_array\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0mdf_s2_chest_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchest_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;31m# Process 'wrist' signals\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[1;32m    776\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    777\u001b[0m             \u001b[0;31m# GH#38939 de facto copy defaults to False only in non-dict cases\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 778\u001b[0;31m             \u001b[0mmgr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict_to_mgr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtyp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmanager\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    779\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMaskedArray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    780\u001b[0m             \u001b[0;32mfrom\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mma\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmrecords\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/internals/construction.py\u001b[0m in \u001b[0;36mdict_to_mgr\u001b[0;34m(data, index, columns, dtype, typ, copy)\u001b[0m\n\u001b[1;32m    501\u001b[0m             \u001b[0marrays\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"dtype\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    502\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 503\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0marrays_to_mgr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtyp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtyp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconsolidate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    504\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    505\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/internals/construction.py\u001b[0m in \u001b[0;36marrays_to_mgr\u001b[0;34m(arrays, columns, index, dtype, verify_integrity, typ, consolidate)\u001b[0m\n\u001b[1;32m    112\u001b[0m         \u001b[0;31m# figure out the index, if necessary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m             \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_extract_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m             \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mensure_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/internals/construction.py\u001b[0m in \u001b[0;36m_extract_index\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m    662\u001b[0m             \u001b[0mraw_lengths\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    663\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mval\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 664\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Per-column arrays must each be 1-dimensional\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    665\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    666\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mindexes\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mraw_lengths\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Per-column arrays must each be 1-dimensional"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8636bc75"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous code failed because some non-accelerometer sensor data (e.g., ECG, EMG) were still 2-dimensional arrays with a single column, which `pd.DataFrame` cannot directly handle as individual columns. I need to flatten these 2D arrays into 1-dimensional arrays before adding them to the `chest_data` and `wrist_data` dictionaries to resolve the `ValueError`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "d2b017ad",
        "outputId": "9bce65f3-b068-4153-8f1b-df08654733c9"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Define the base path to the S2 subject data directory\n",
        "s2_data_path = '/content/WESAD_data/WESAD/S2'\n",
        "\n",
        "# Load the S2.pkl file into a dictionary first\n",
        "s2_raw_data_dict = pd.read_pickle(f'{s2_data_path}/S2.pkl')\n",
        "\n",
        "# Process 'chest' signals\n",
        "chest_signals = s2_raw_data_dict['signal']['chest']\n",
        "chest_data = {}\n",
        "for sensor_name, data_array in chest_signals.items():\n",
        "    if sensor_name == 'ACC':\n",
        "        # Accelerometer data is typically 3-axis, so create three columns\n",
        "        chest_data['ACC_x'] = data_array[:, 0]\n",
        "        chest_data['ACC_y'] = data_array[:, 1]\n",
        "        chest_data['ACC_z'] = data_array[:, 2]\n",
        "    else:\n",
        "        # Other sensors might be 2D arrays with a single column, flatten them\n",
        "        chest_data[sensor_name] = data_array.flatten()\n",
        "\n",
        "df_s2_chest_data = pd.DataFrame(chest_data)\n",
        "\n",
        "# Process 'wrist' signals\n",
        "wrist_signals = s2_raw_data_dict['signal']['wrist']\n",
        "wrist_data = {}\n",
        "for sensor_name, data_array in wrist_signals.items():\n",
        "    if sensor_name == 'ACC':\n",
        "        # Accelerometer data is typically 3-axis, so create three columns\n",
        "        wrist_data['ACC_x'] = data_array[:, 0]\n",
        "        wrist_data['ACC_y'] = data_array[:, 1]\n",
        "        wrist_data['ACC_z'] = data_array[:, 2]\n",
        "    else:\n",
        "        # Other sensors might be 2D arrays with a single column, flatten them\n",
        "        wrist_data[sensor_name] = data_array.flatten()\n",
        "\n",
        "df_s2_wrist_data = pd.DataFrame(wrist_data)\n",
        "\n",
        "# Load the S2_quest.csv file into a pandas DataFrame\n",
        "df_s2_quest = pd.read_csv(f'{s2_data_path}/S2_quest.csv')\n",
        "\n",
        "print(\"df_s2_chest_data head:\")\n",
        "print(df_s2_chest_data.head())\n",
        "\n",
        "print(\"\\ndf_s2_wrist_data head:\")\n",
        "print(df_s2_wrist_data.head())\n",
        "\n",
        "print(\"\\ndf_s2_quest head:\")\n",
        "print(df_s2_quest.head())"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "All arrays must be of the same length",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2549989486.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0mwrist_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msensor_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_array\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m \u001b[0mdf_s2_wrist_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwrist_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;31m# Load the S2_quest.csv file into a pandas DataFrame\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[1;32m    776\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    777\u001b[0m             \u001b[0;31m# GH#38939 de facto copy defaults to False only in non-dict cases\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 778\u001b[0;31m             \u001b[0mmgr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict_to_mgr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtyp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmanager\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    779\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMaskedArray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    780\u001b[0m             \u001b[0;32mfrom\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mma\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmrecords\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/internals/construction.py\u001b[0m in \u001b[0;36mdict_to_mgr\u001b[0;34m(data, index, columns, dtype, typ, copy)\u001b[0m\n\u001b[1;32m    501\u001b[0m             \u001b[0marrays\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"dtype\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    502\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 503\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0marrays_to_mgr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtyp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtyp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconsolidate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    504\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    505\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/internals/construction.py\u001b[0m in \u001b[0;36marrays_to_mgr\u001b[0;34m(arrays, columns, index, dtype, verify_integrity, typ, consolidate)\u001b[0m\n\u001b[1;32m    112\u001b[0m         \u001b[0;31m# figure out the index, if necessary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m             \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_extract_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m             \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mensure_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/internals/construction.py\u001b[0m in \u001b[0;36m_extract_index\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m    675\u001b[0m         \u001b[0mlengths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_lengths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    676\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlengths\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 677\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"All arrays must be of the same length\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    678\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    679\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhave_dicts\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: All arrays must be of the same length"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b3c9fde6"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous code failed because sensor data arrays within 'wrist' (and likely 'chest') have different lengths due to varying sampling rates, making it impossible to directly create a single pandas DataFrame. To resolve the `ValueError: All arrays must be of the same length`, I will create separate DataFrames for each individual sensor signal (e.g., 'ACC', 'BVP', 'EDA', 'TEMP', 'ECG', 'EMG') and store them in dictionaries (`chest_dataframes` and `wrist_dataframes`) for organized access.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c05627fe",
        "outputId": "7a5635c7-dbb4-4ebc-abae-9267442c6583"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Define the base path to the S2 subject data directory\n",
        "s2_data_path = '/content/WESAD_data/WESAD/S2'\n",
        "\n",
        "# Load the S2.pkl file into a dictionary first\n",
        "s2_raw_data_dict = pd.read_pickle(f'{s2_data_path}/S2.pkl')\n",
        "\n",
        "# Initialize dictionaries to hold individual sensor DataFrames\n",
        "chest_dataframes = {}\n",
        "wrist_dataframes = {}\n",
        "\n",
        "# Process 'chest' signals\n",
        "chest_signals = s2_raw_data_dict['signal']['chest']\n",
        "for sensor_name, data_array in chest_signals.items():\n",
        "    if sensor_name == 'ACC':\n",
        "        # Accelerometer data is 3-axis, create three columns\n",
        "        chest_dataframes[sensor_name] = pd.DataFrame(data_array, columns=['ACC_x', 'ACC_y', 'ACC_z'])\n",
        "    else:\n",
        "        # Other sensors are typically 1D or 2D with one column, flatten and create a DataFrame\n",
        "        chest_dataframes[sensor_name] = pd.DataFrame(data_array.flatten(), columns=[sensor_name])\n",
        "\n",
        "# Process 'wrist' signals\n",
        "wrist_signals = s2_raw_data_dict['signal']['wrist']\n",
        "for sensor_name, data_array in wrist_signals.items():\n",
        "    if sensor_name == 'ACC':\n",
        "        # Accelerometer data is 3-axis, create three columns\n",
        "        wrist_dataframes[sensor_name] = pd.DataFrame(data_array, columns=['ACC_x', 'ACC_y', 'ACC_z'])\n",
        "    else:\n",
        "        # Other sensors are typically 1D or 2D with one column, flatten and create a DataFrame\n",
        "        wrist_dataframes[sensor_name] = pd.DataFrame(data_array.flatten(), columns=[sensor_name])\n",
        "\n",
        "# Load the S2_quest.csv file into a pandas DataFrame\n",
        "df_s2_quest = pd.read_csv(f'{s2_data_path}/S2_quest.csv')\n",
        "\n",
        "print(\"Chest DataFrames (first 5 rows of each):\")\n",
        "for sensor_name, df in chest_dataframes.items():\n",
        "    print(f\"\\n{sensor_name}:\")\n",
        "    print(df.head())\n",
        "\n",
        "print(\"\\nWrist DataFrames (first 5 rows of each):\")\n",
        "for sensor_name, df in wrist_dataframes.items():\n",
        "    print(f\"\\n{sensor_name}:\")\n",
        "    print(df.head())\n",
        "\n",
        "print(\"\\ndf_s2_quest head:\")\n",
        "print(df_s2_quest.head())"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chest DataFrames (first 5 rows of each):\n",
            "\n",
            "ACC:\n",
            "    ACC_x   ACC_y   ACC_z\n",
            "0  0.9554 -0.2220 -0.5580\n",
            "1  0.9258 -0.2216 -0.5538\n",
            "2  0.9082 -0.2196 -0.5392\n",
            "3  0.8974 -0.2102 -0.5122\n",
            "4  0.8882 -0.2036 -0.4824\n",
            "\n",
            "ECG:\n",
            "        ECG\n",
            "0  0.021423\n",
            "1  0.020325\n",
            "2  0.016525\n",
            "3  0.016708\n",
            "4  0.011673\n",
            "\n",
            "EMG:\n",
            "        EMG\n",
            "0 -0.004440\n",
            "1  0.004349\n",
            "2  0.005173\n",
            "3  0.007187\n",
            "4 -0.015152\n",
            "\n",
            "EDA:\n",
            "        EDA\n",
            "0  5.250549\n",
            "1  5.267334\n",
            "2  5.243301\n",
            "3  5.249405\n",
            "4  5.286407\n",
            "\n",
            "Temp:\n",
            "        Temp\n",
            "0  30.120758\n",
            "1  30.129517\n",
            "2  30.138214\n",
            "3  30.129517\n",
            "4  30.130951\n",
            "\n",
            "Resp:\n",
            "       Resp\n",
            "0 -1.148987\n",
            "1 -1.124573\n",
            "2 -1.152039\n",
            "3 -1.158142\n",
            "4 -1.161194\n",
            "\n",
            "Wrist DataFrames (first 5 rows of each):\n",
            "\n",
            "ACC:\n",
            "   ACC_x  ACC_y  ACC_z\n",
            "0   62.0  -21.0  107.0\n",
            "1   66.0   13.0   53.0\n",
            "2   41.0    9.0   15.0\n",
            "3   52.0   16.0   24.0\n",
            "4   54.0   15.0   34.0\n",
            "\n",
            "BVP:\n",
            "     BVP\n",
            "0 -59.37\n",
            "1 -53.42\n",
            "2 -44.40\n",
            "3 -33.17\n",
            "4 -20.79\n",
            "\n",
            "EDA:\n",
            "        EDA\n",
            "0  1.138257\n",
            "1  1.125444\n",
            "2  1.011405\n",
            "3  1.033188\n",
            "4  0.935807\n",
            "\n",
            "TEMP:\n",
            "    TEMP\n",
            "0  35.41\n",
            "1  35.41\n",
            "2  35.41\n",
            "3  35.41\n",
            "4  35.41\n",
            "\n",
            "df_s2_quest head:\n",
            "                  # Subj;S2;;;;;;;;;;;;;;;;;;;;;;;;;\n",
            "0  # ORDER;Base;TSST;Medi 1;Fun;Medi 2;sRead;fRea...\n",
            "1  # START;7.08;39.55;70.19;81.25;93.38;54.42;89....\n",
            "2  # END;26.32;50.3;77.1;87.47;100.15;56.07;91.15...\n",
            "3                         ;;;;;;;;;;;;;;;;;;;;;;;;;;\n",
            "4  # PANAS;1;1;3;2;1;3;1;1;1;2;2;2;2;1;4;3;4;4;2;...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b0b99604"
      },
      "source": [
        "## Initial Data Inspection\n",
        "\n",
        "### Subtask:\n",
        "Perform an initial inspection of the loaded data, including checking the first few rows (.head()), data types (.info()), and descriptive statistics (.describe()) to identify potential issues.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7daef674"
      },
      "source": [
        "**Reasoning**:\n",
        "To perform an initial inspection, I will iterate through the `chest_dataframes` and `wrist_dataframes` dictionaries, applying `.head()`, `.info()`, and `.describe()` to each sensor DataFrame. I will also apply these methods to the `df_s2_quest` DataFrame.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3687c834",
        "outputId": "7b963909-4dd0-477c-d0e6-407607a3ba12"
      },
      "source": [
        "print(\"--- Initial Data Inspection for Chest Sensor DataFrames ---\")\n",
        "for sensor_name, df in chest_dataframes.items():\n",
        "    print(f\"\\n----- {sensor_name} Data (Chest) -----\")\n",
        "    print(f\"\\n{sensor_name}.head():\")\n",
        "    print(df.head())\n",
        "    print(f\"\\n{sensor_name}.info():\")\n",
        "    df.info()\n",
        "    print(f\"\\n{sensor_name}.describe():\")\n",
        "    print(df.describe())\n",
        "\n",
        "print(\"\\n--- Initial Data Inspection for Wrist Sensor DataFrames ---\")\n",
        "for sensor_name, df in wrist_dataframes.items():\n",
        "    print(f\"\\n----- {sensor_name} Data (Wrist) -----\")\n",
        "    print(f\"\\n{sensor_name}.head():\")\n",
        "    print(df.head())\n",
        "    print(f\"\\n{sensor_name}.info():\")\n",
        "    df.info()\n",
        "    print(f\"\\n{sensor_name}.describe():\")\n",
        "    print(df.describe())\n",
        "\n",
        "print(\"\\n--- Initial Data Inspection for Questionnaire Data (df_s2_quest) ---\")\n",
        "print(\"\\ndf_s2_quest.head():\")\n",
        "print(df_s2_quest.head())\n",
        "print(\"\\ndf_s2_quest.info():\")\n",
        "df_s2_quest.info()\n",
        "print(\"\\ndf_s2_quest.describe():\")\n",
        "print(df_s2_quest.describe())"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Initial Data Inspection for Chest Sensor DataFrames ---\n",
            "\n",
            "----- ACC Data (Chest) -----\n",
            "\n",
            "ACC.head():\n",
            "    ACC_x   ACC_y   ACC_z\n",
            "0  0.9554 -0.2220 -0.5580\n",
            "1  0.9258 -0.2216 -0.5538\n",
            "2  0.9082 -0.2196 -0.5392\n",
            "3  0.8974 -0.2102 -0.5122\n",
            "4  0.8882 -0.2036 -0.4824\n",
            "\n",
            "ACC.info():\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 4255300 entries, 0 to 4255299\n",
            "Data columns (total 3 columns):\n",
            " #   Column  Dtype  \n",
            "---  ------  -----  \n",
            " 0   ACC_x   float64\n",
            " 1   ACC_y   float64\n",
            " 2   ACC_z   float64\n",
            "dtypes: float64(3)\n",
            "memory usage: 97.4 MB\n",
            "\n",
            "ACC.describe():\n",
            "              ACC_x         ACC_y         ACC_z\n",
            "count  4.255300e+06  4.255300e+06  4.255300e+06\n",
            "mean   7.538656e-01 -1.331884e-01 -4.226812e-01\n",
            "std    1.751162e-01  6.417297e-02  3.002357e-01\n",
            "min    2.150000e-01 -6.600000e-01 -1.135400e+00\n",
            "25%    6.210001e-01 -1.498000e-01 -7.172000e-01\n",
            "50%    8.568000e-01 -1.256000e-01 -3.222000e-01\n",
            "75%    8.922000e-01 -9.579998e-02 -2.098000e-01\n",
            "max    2.029800e+00  5.390000e-01  1.246800e+00\n",
            "\n",
            "----- ECG Data (Chest) -----\n",
            "\n",
            "ECG.head():\n",
            "        ECG\n",
            "0  0.021423\n",
            "1  0.020325\n",
            "2  0.016525\n",
            "3  0.016708\n",
            "4  0.011673\n",
            "\n",
            "ECG.info():\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 4255300 entries, 0 to 4255299\n",
            "Data columns (total 1 columns):\n",
            " #   Column  Dtype  \n",
            "---  ------  -----  \n",
            " 0   ECG     float64\n",
            "dtypes: float64(1)\n",
            "memory usage: 32.5 MB\n",
            "\n",
            "ECG.describe():\n",
            "                ECG\n",
            "count  4.255300e+06\n",
            "mean   1.212245e-03\n",
            "std    1.540773e-01\n",
            "min   -1.499542e+00\n",
            "25%   -6.797791e-02\n",
            "50%    8.239746e-03\n",
            "75%    4.435730e-02\n",
            "max    1.499313e+00\n",
            "\n",
            "----- EMG Data (Chest) -----\n",
            "\n",
            "EMG.head():\n",
            "        EMG\n",
            "0 -0.004440\n",
            "1  0.004349\n",
            "2  0.005173\n",
            "3  0.007187\n",
            "4 -0.015152\n",
            "\n",
            "EMG.info():\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 4255300 entries, 0 to 4255299\n",
            "Data columns (total 1 columns):\n",
            " #   Column  Dtype  \n",
            "---  ------  -----  \n",
            " 0   EMG     float64\n",
            "dtypes: float64(1)\n",
            "memory usage: 32.5 MB\n",
            "\n",
            "EMG.describe():\n",
            "                EMG\n",
            "count  4.255300e+06\n",
            "mean  -2.412918e-03\n",
            "std    1.165187e-02\n",
            "min   -4.149628e-01\n",
            "25%   -8.331299e-03\n",
            "50%   -1.602173e-03\n",
            "75%    3.982544e-03\n",
            "max    3.009796e-01\n",
            "\n",
            "----- EDA Data (Chest) -----\n",
            "\n",
            "EDA.head():\n",
            "        EDA\n",
            "0  5.250549\n",
            "1  5.267334\n",
            "2  5.243301\n",
            "3  5.249405\n",
            "4  5.286407\n",
            "\n",
            "EDA.info():\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 4255300 entries, 0 to 4255299\n",
            "Data columns (total 1 columns):\n",
            " #   Column  Dtype  \n",
            "---  ------  -----  \n",
            " 0   EDA     float64\n",
            "dtypes: float64(1)\n",
            "memory usage: 32.5 MB\n",
            "\n",
            "EDA.describe():\n",
            "                EDA\n",
            "count  4.255300e+06\n",
            "mean   1.528387e+00\n",
            "std    1.238425e+00\n",
            "min    2.632141e-01\n",
            "25%    7.995605e-01\n",
            "50%    1.163101e+00\n",
            "75%    1.546478e+00\n",
            "max    7.576752e+00\n",
            "\n",
            "----- Temp Data (Chest) -----\n",
            "\n",
            "Temp.head():\n",
            "        Temp\n",
            "0  30.120758\n",
            "1  30.129517\n",
            "2  30.138214\n",
            "3  30.129517\n",
            "4  30.130951\n",
            "\n",
            "Temp.info():\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 4255300 entries, 0 to 4255299\n",
            "Data columns (total 1 columns):\n",
            " #   Column  Dtype  \n",
            "---  ------  -----  \n",
            " 0   Temp    float32\n",
            "dtypes: float32(1)\n",
            "memory usage: 16.2 MB\n",
            "\n",
            "Temp.describe():\n",
            "               Temp\n",
            "count  4.255300e+06\n",
            "mean   3.087667e+01\n",
            "std    1.330574e+00\n",
            "min    2.804526e+01\n",
            "25%    3.057495e+01\n",
            "50%    3.128992e+01\n",
            "75%    3.181512e+01\n",
            "max    3.437039e+01\n",
            "\n",
            "----- Resp Data (Chest) -----\n",
            "\n",
            "Resp.head():\n",
            "       Resp\n",
            "0 -1.148987\n",
            "1 -1.124573\n",
            "2 -1.152039\n",
            "3 -1.158142\n",
            "4 -1.161194\n",
            "\n",
            "Resp.info():\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 4255300 entries, 0 to 4255299\n",
            "Data columns (total 1 columns):\n",
            " #   Column  Dtype  \n",
            "---  ------  -----  \n",
            " 0   Resp    float64\n",
            "dtypes: float64(1)\n",
            "memory usage: 32.5 MB\n",
            "\n",
            "Resp.describe():\n",
            "               Resp\n",
            "count  4.255300e+06\n",
            "mean   5.490614e-02\n",
            "std    2.917749e+00\n",
            "min   -2.790375e+01\n",
            "25%   -1.481628e+00\n",
            "50%   -1.571655e-01\n",
            "75%    1.618958e+00\n",
            "max    2.737885e+01\n",
            "\n",
            "--- Initial Data Inspection for Wrist Sensor DataFrames ---\n",
            "\n",
            "----- ACC Data (Wrist) -----\n",
            "\n",
            "ACC.head():\n",
            "   ACC_x  ACC_y  ACC_z\n",
            "0   62.0  -21.0  107.0\n",
            "1   66.0   13.0   53.0\n",
            "2   41.0    9.0   15.0\n",
            "3   52.0   16.0   24.0\n",
            "4   54.0   15.0   34.0\n",
            "\n",
            "ACC.info():\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 194528 entries, 0 to 194527\n",
            "Data columns (total 3 columns):\n",
            " #   Column  Non-Null Count   Dtype  \n",
            "---  ------  --------------   -----  \n",
            " 0   ACC_x   194528 non-null  float64\n",
            " 1   ACC_y   194528 non-null  float64\n",
            " 2   ACC_z   194528 non-null  float64\n",
            "dtypes: float64(3)\n",
            "memory usage: 4.5 MB\n",
            "\n",
            "ACC.describe():\n",
            "               ACC_x          ACC_y          ACC_z\n",
            "count  194528.000000  194528.000000  194528.000000\n",
            "mean       39.701452      -0.407381      18.990526\n",
            "std        19.520898      33.206671      25.003502\n",
            "min      -128.000000    -128.000000    -128.000000\n",
            "25%        21.000000     -12.000000       9.000000\n",
            "50%        44.000000       5.000000      17.000000\n",
            "75%        57.000000      23.000000      37.000000\n",
            "max       127.000000     104.000000     127.000000\n",
            "\n",
            "----- BVP Data (Wrist) -----\n",
            "\n",
            "BVP.head():\n",
            "     BVP\n",
            "0 -59.37\n",
            "1 -53.42\n",
            "2 -44.40\n",
            "3 -33.17\n",
            "4 -20.79\n",
            "\n",
            "BVP.info():\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 389056 entries, 0 to 389055\n",
            "Data columns (total 1 columns):\n",
            " #   Column  Non-Null Count   Dtype  \n",
            "---  ------  --------------   -----  \n",
            " 0   BVP     389056 non-null  float64\n",
            "dtypes: float64(1)\n",
            "memory usage: 3.0 MB\n",
            "\n",
            "BVP.describe():\n",
            "                 BVP\n",
            "count  389056.000000\n",
            "mean       -0.000427\n",
            "std        75.871334\n",
            "min      -873.670000\n",
            "25%       -19.670000\n",
            "50%         3.010000\n",
            "75%        19.032500\n",
            "max       988.080000\n",
            "\n",
            "----- EDA Data (Wrist) -----\n",
            "\n",
            "EDA.head():\n",
            "        EDA\n",
            "0  1.138257\n",
            "1  1.125444\n",
            "2  1.011405\n",
            "3  1.033188\n",
            "4  0.935807\n",
            "\n",
            "EDA.info():\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 24316 entries, 0 to 24315\n",
            "Data columns (total 1 columns):\n",
            " #   Column  Non-Null Count  Dtype  \n",
            "---  ------  --------------  -----  \n",
            " 0   EDA     24316 non-null  float64\n",
            "dtypes: float64(1)\n",
            "memory usage: 190.1 KB\n",
            "\n",
            "EDA.describe():\n",
            "                EDA\n",
            "count  24316.000000\n",
            "mean       0.391743\n",
            "std        0.329236\n",
            "min        0.045113\n",
            "25%        0.169403\n",
            "50%        0.236032\n",
            "75%        0.520487\n",
            "max        1.717419\n",
            "\n",
            "----- TEMP Data (Wrist) -----\n",
            "\n",
            "TEMP.head():\n",
            "    TEMP\n",
            "0  35.41\n",
            "1  35.41\n",
            "2  35.41\n",
            "3  35.41\n",
            "4  35.41\n",
            "\n",
            "TEMP.info():\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 24316 entries, 0 to 24315\n",
            "Data columns (total 1 columns):\n",
            " #   Column  Non-Null Count  Dtype  \n",
            "---  ------  --------------  -----  \n",
            " 0   TEMP    24316 non-null  float64\n",
            "dtypes: float64(1)\n",
            "memory usage: 190.1 KB\n",
            "\n",
            "TEMP.describe():\n",
            "               TEMP\n",
            "count  24316.000000\n",
            "mean      34.386945\n",
            "std        1.045340\n",
            "min       32.310000\n",
            "25%       33.680000\n",
            "50%       34.250000\n",
            "75%       35.410000\n",
            "max       35.970000\n",
            "\n",
            "--- Initial Data Inspection for Questionnaire Data (df_s2_quest) ---\n",
            "\n",
            "df_s2_quest.head():\n",
            "                  # Subj;S2;;;;;;;;;;;;;;;;;;;;;;;;;\n",
            "0  # ORDER;Base;TSST;Medi 1;Fun;Medi 2;sRead;fRea...\n",
            "1  # START;7.08;39.55;70.19;81.25;93.38;54.42;89....\n",
            "2  # END;26.32;50.3;77.1;87.47;100.15;56.07;91.15...\n",
            "3                         ;;;;;;;;;;;;;;;;;;;;;;;;;;\n",
            "4  # PANAS;1;1;3;2;1;3;1;1;1;2;2;2;2;1;4;3;4;4;2;...\n",
            "\n",
            "df_s2_quest.info():\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 23 entries, 0 to 22\n",
            "Data columns (total 1 columns):\n",
            " #   Column                              Non-Null Count  Dtype \n",
            "---  ------                              --------------  ----- \n",
            " 0   # Subj;S2;;;;;;;;;;;;;;;;;;;;;;;;;  23 non-null     object\n",
            "dtypes: object(1)\n",
            "memory usage: 316.0+ bytes\n",
            "\n",
            "df_s2_quest.describe():\n",
            "       # Subj;S2;;;;;;;;;;;;;;;;;;;;;;;;;\n",
            "count                                  23\n",
            "unique                                 18\n",
            "top            ;;;;;;;;;;;;;;;;;;;;;;;;;;\n",
            "freq                                    4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "65039e93"
      },
      "source": [
        "## Handle Missing Values\n",
        "\n",
        "### Subtask:\n",
        "Identify and address any missing values in the dataset. This may involve imputation, deletion of rows/columns, or other appropriate strategies based on the extent and nature of missing data.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9a7363d1"
      },
      "source": [
        "**Reasoning**:\n",
        "To identify missing values, I will iterate through each DataFrame in the `chest_dataframes` and `wrist_dataframes` dictionaries and print the sum of null values for each column. I will also add a markdown block to acknowledge that `df_s2_quest` needs parsing before checking for missing values.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "68f3c6ca",
        "outputId": "54e06b03-f056-4cb3-bc9a-0584f4ce6b73"
      },
      "source": [
        "print(\"--- Checking for Missing Values in Chest Sensor DataFrames ---\")\n",
        "for sensor_name, df in chest_dataframes.items():\n",
        "    print(f\"\\nMissing values in {sensor_name} (Chest):\")\n",
        "    print(df.isnull().sum())\n",
        "\n",
        "print(\"\\n--- Checking for Missing Values in Wrist Sensor DataFrames ---\")\n",
        "for sensor_name, df in wrist_dataframes.items():\n",
        "    print(f\"\\nMissing values in {sensor_name} (Wrist):\")\n",
        "    print(df.isnull().sum())\n"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Checking for Missing Values in Chest Sensor DataFrames ---\n",
            "\n",
            "Missing values in ACC (Chest):\n",
            "ACC_x    0\n",
            "ACC_y    0\n",
            "ACC_z    0\n",
            "dtype: int64\n",
            "\n",
            "Missing values in ECG (Chest):\n",
            "ECG    0\n",
            "dtype: int64\n",
            "\n",
            "Missing values in EMG (Chest):\n",
            "EMG    0\n",
            "dtype: int64\n",
            "\n",
            "Missing values in EDA (Chest):\n",
            "EDA    0\n",
            "dtype: int64\n",
            "\n",
            "Missing values in Temp (Chest):\n",
            "Temp    0\n",
            "dtype: int64\n",
            "\n",
            "Missing values in Resp (Chest):\n",
            "Resp    0\n",
            "dtype: int64\n",
            "\n",
            "--- Checking for Missing Values in Wrist Sensor DataFrames ---\n",
            "\n",
            "Missing values in ACC (Wrist):\n",
            "ACC_x    0\n",
            "ACC_y    0\n",
            "ACC_z    0\n",
            "dtype: int64\n",
            "\n",
            "Missing values in BVP (Wrist):\n",
            "BVP    0\n",
            "dtype: int64\n",
            "\n",
            "Missing values in EDA (Wrist):\n",
            "EDA    0\n",
            "dtype: int64\n",
            "\n",
            "Missing values in TEMP (Wrist):\n",
            "TEMP    0\n",
            "dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "38cd41c5"
      },
      "source": [
        "```markdown\n",
        "**Regarding `df_s2_quest`:**\n",
        "\n",
        "The `df_s2_quest` DataFrame is not yet in a tabular format suitable for a direct `.isnull().sum()` check. It contains metadata and questionnaire responses within a few rows, separated by semicolons. This DataFrame will require further parsing and cleaning before a meaningful assessment of missing values can be performed for individual questions/fields.\n",
        "\n",
        "Based on the current state of the chest and wrist sensor data, there are no missing values detected.\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ee336d69"
      },
      "source": [
        "## Handle Duplicates\n",
        "\n",
        "### Subtask:\n",
        "Check for and remove any duplicate rows in the dataset to ensure data integrity.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a3de8ef4"
      },
      "source": [
        "**Reasoning**:\n",
        "To identify duplicate rows, I will iterate through each DataFrame in the `chest_dataframes` and `wrist_dataframes` dictionaries and use the `.duplicated().sum()` method to count and print the number of duplicate rows for each sensor.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3df1e040",
        "outputId": "4f2de2d8-d593-4bd7-a4fe-c0fe70abbb4a"
      },
      "source": [
        "print(\"--- Checking for Duplicate Rows in Chest Sensor DataFrames ---\")\n",
        "for sensor_name, df in chest_dataframes.items():\n",
        "    print(f\"\\nDuplicate rows in {sensor_name} (Chest): {df.duplicated().sum()}\")\n",
        "\n",
        "print(\"\\n--- Checking for Duplicate Rows in Wrist Sensor DataFrames ---\")\n",
        "for sensor_name, df in wrist_dataframes.items():\n",
        "    print(f\"\\nDuplicate rows in {sensor_name} (Wrist): {df.duplicated().sum()}\")"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Checking for Duplicate Rows in Chest Sensor DataFrames ---\n",
            "\n",
            "Duplicate rows in ACC (Chest): 470641\n",
            "\n",
            "Duplicate rows in ECG (Chest): 4214034\n",
            "\n",
            "Duplicate rows in EMG (Chest): 4250976\n",
            "\n",
            "Duplicate rows in EDA (Chest): 4236717\n",
            "\n",
            "Duplicate rows in Temp (Chest): 4251080\n",
            "\n",
            "Duplicate rows in Resp (Chest): 4228466\n",
            "\n",
            "--- Checking for Duplicate Rows in Wrist Sensor DataFrames ---\n",
            "\n",
            "Duplicate rows in ACC (Wrist): 149038\n",
            "\n",
            "Duplicate rows in BVP (Wrist): 343616\n",
            "\n",
            "Duplicate rows in EDA (Wrist): 22615\n",
            "\n",
            "Duplicate rows in TEMP (Wrist): 24133\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0d5b5d32"
      },
      "source": [
        "```markdown\n",
        "**Regarding `df_s2_quest`:**\n",
        "\n",
        "The `df_s2_quest` DataFrame is not yet in a tabular format suitable for a direct `.duplicated().sum()` check. It contains metadata and questionnaire responses within a few rows, separated by semicolons. This DataFrame will require further parsing and cleaning before a meaningful assessment of duplicate rows can be performed for individual questions/fields.\n",
        "\n",
        "**Regarding Sensor DataFrames:**\n",
        "\n",
        "Many of the chest and wrist sensor DataFrames contain a significant number of duplicate rows. This will need to be addressed in the next step to ensure data integrity and prevent skewed analysis.\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "97ab4e54"
      },
      "source": [
        "**Regarding `df_s2_quest`:**\n",
        "\n",
        "The `df_s2_quest` DataFrame is not yet in a tabular format suitable for a direct `.duplicated().sum()` check. It contains metadata and questionnaire responses within a few rows, separated by semicolons. This DataFrame will require further parsing and cleaning before a meaningful assessment of duplicate rows can be performed for individual questions/fields.\n",
        "\n",
        "**Regarding Sensor DataFrames:**\n",
        "\n",
        "Many of the chest and wrist sensor DataFrames contain a significant number of duplicate rows. This will need to be addressed in the next step to ensure data integrity and prevent skewed analysis."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "47c12422"
      },
      "source": [
        "**Reasoning**:\n",
        "Many sensor dataframes were found to contain duplicate rows, which can affect data integrity. To address this, I will iterate through each DataFrame in the `chest_dataframes` and `wrist_dataframes` dictionaries and use the `.drop_duplicates()` method to remove all duplicate rows. After removal, I will confirm by re-checking the count of duplicate rows for each DataFrame.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1f37cf66",
        "outputId": "49cc8c74-7bbf-447d-f588-28c363db85d2"
      },
      "source": [
        "print(\"--- Removing Duplicate Rows from Chest Sensor DataFrames ---\")\n",
        "for sensor_name, df in chest_dataframes.items():\n",
        "    initial_duplicates = df.duplicated().sum()\n",
        "    if initial_duplicates > 0:\n",
        "        chest_dataframes[sensor_name] = df.drop_duplicates().reset_index(drop=True)\n",
        "        print(f\"Removed {initial_duplicates} duplicate rows from {sensor_name} (Chest). New count: {chest_dataframes[sensor_name].duplicated().sum()}\")\n",
        "    else:\n",
        "        print(f\"No duplicate rows found in {sensor_name} (Chest).\")\n",
        "\n",
        "print(\"\\n--- Removing Duplicate Rows from Wrist Sensor DataFrames ---\")\n",
        "for sensor_name, df in wrist_dataframes.items():\n",
        "    initial_duplicates = df.duplicated().sum()\n",
        "    if initial_duplicates > 0:\n",
        "        wrist_dataframes[sensor_name] = df.drop_duplicates().reset_index(drop=True)\n",
        "        print(f\"Removed {initial_duplicates} duplicate rows from {sensor_name} (Wrist). New count: {wrist_dataframes[sensor_name].duplicated().sum()}\")\n",
        "    else:\n",
        "        print(f\"No duplicate rows found in {sensor_name} (Wrist).\")"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Removing Duplicate Rows from Chest Sensor DataFrames ---\n",
            "Removed 470641 duplicate rows from ACC (Chest). New count: 0\n",
            "Removed 4214034 duplicate rows from ECG (Chest). New count: 0\n",
            "Removed 4250976 duplicate rows from EMG (Chest). New count: 0\n",
            "Removed 4236717 duplicate rows from EDA (Chest). New count: 0\n",
            "Removed 4251080 duplicate rows from Temp (Chest). New count: 0\n",
            "Removed 4228466 duplicate rows from Resp (Chest). New count: 0\n",
            "\n",
            "--- Removing Duplicate Rows from Wrist Sensor DataFrames ---\n",
            "Removed 149038 duplicate rows from ACC (Wrist). New count: 0\n",
            "Removed 343616 duplicate rows from BVP (Wrist). New count: 0\n",
            "Removed 22615 duplicate rows from EDA (Wrist). New count: 0\n",
            "Removed 24133 duplicate rows from TEMP (Wrist). New count: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e78b71f6"
      },
      "source": [
        "## Check Data Types\n",
        "\n",
        "### Subtask:\n",
        "Verify that all columns have appropriate data types (e.g., numeric, categorical, datetime) and convert them if necessary to ensure correct processing in subsequent analysis.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f372265b"
      },
      "source": [
        "**Reasoning**:\n",
        "To verify data types, I will iterate through each DataFrame in `chest_dataframes` and `wrist_dataframes` and print their data types and information using `.info()`. I will also print the `.info()` for `df_s2_quest` and acknowledge that it requires parsing.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cf215c17",
        "outputId": "38c79e60-125c-443d-cac1-ebcc21832915"
      },
      "source": [
        "print(\"--- Checking Data Types for Chest Sensor DataFrames ---\")\n",
        "for sensor_name, df in chest_dataframes.items():\n",
        "    print(f\"\\n----- {sensor_name} Data (Chest) -----\")\n",
        "    df.info()\n",
        "\n",
        "print(\"\\n--- Checking Data Types for Wrist Sensor DataFrames ---\")\n",
        "for sensor_name, df in wrist_dataframes.items():\n",
        "    print(f\"\\n----- {sensor_name} Data (Wrist) -----\")\n",
        "    df.info()\n",
        "\n",
        "print(\"\\n--- Checking Data Types for Questionnaire Data (df_s2_quest) ---\")\n",
        "df_s2_quest.info()"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Checking Data Types for Chest Sensor DataFrames ---\n",
            "\n",
            "----- ACC Data (Chest) -----\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 3784659 entries, 0 to 3784658\n",
            "Data columns (total 3 columns):\n",
            " #   Column  Dtype  \n",
            "---  ------  -----  \n",
            " 0   ACC_x   float64\n",
            " 1   ACC_y   float64\n",
            " 2   ACC_z   float64\n",
            "dtypes: float64(3)\n",
            "memory usage: 86.6 MB\n",
            "\n",
            "----- ECG Data (Chest) -----\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 41266 entries, 0 to 41265\n",
            "Data columns (total 1 columns):\n",
            " #   Column  Non-Null Count  Dtype  \n",
            "---  ------  --------------  -----  \n",
            " 0   ECG     41266 non-null  float64\n",
            "dtypes: float64(1)\n",
            "memory usage: 322.5 KB\n",
            "\n",
            "----- EMG Data (Chest) -----\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 4324 entries, 0 to 4323\n",
            "Data columns (total 1 columns):\n",
            " #   Column  Non-Null Count  Dtype  \n",
            "---  ------  --------------  -----  \n",
            " 0   EMG     4324 non-null   float64\n",
            "dtypes: float64(1)\n",
            "memory usage: 33.9 KB\n",
            "\n",
            "----- EDA Data (Chest) -----\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 18583 entries, 0 to 18582\n",
            "Data columns (total 1 columns):\n",
            " #   Column  Non-Null Count  Dtype  \n",
            "---  ------  --------------  -----  \n",
            " 0   EDA     18583 non-null  float64\n",
            "dtypes: float64(1)\n",
            "memory usage: 145.3 KB\n",
            "\n",
            "----- Temp Data (Chest) -----\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 4220 entries, 0 to 4219\n",
            "Data columns (total 1 columns):\n",
            " #   Column  Non-Null Count  Dtype  \n",
            "---  ------  --------------  -----  \n",
            " 0   Temp    4220 non-null   float32\n",
            "dtypes: float32(1)\n",
            "memory usage: 16.6 KB\n",
            "\n",
            "----- Resp Data (Chest) -----\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 26834 entries, 0 to 26833\n",
            "Data columns (total 1 columns):\n",
            " #   Column  Non-Null Count  Dtype  \n",
            "---  ------  --------------  -----  \n",
            " 0   Resp    26834 non-null  float64\n",
            "dtypes: float64(1)\n",
            "memory usage: 209.8 KB\n",
            "\n",
            "--- Checking Data Types for Wrist Sensor DataFrames ---\n",
            "\n",
            "----- ACC Data (Wrist) -----\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 45490 entries, 0 to 45489\n",
            "Data columns (total 3 columns):\n",
            " #   Column  Non-Null Count  Dtype  \n",
            "---  ------  --------------  -----  \n",
            " 0   ACC_x   45490 non-null  float64\n",
            " 1   ACC_y   45490 non-null  float64\n",
            " 2   ACC_z   45490 non-null  float64\n",
            "dtypes: float64(3)\n",
            "memory usage: 1.0 MB\n",
            "\n",
            "----- BVP Data (Wrist) -----\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 45440 entries, 0 to 45439\n",
            "Data columns (total 1 columns):\n",
            " #   Column  Non-Null Count  Dtype  \n",
            "---  ------  --------------  -----  \n",
            " 0   BVP     45440 non-null  float64\n",
            "dtypes: float64(1)\n",
            "memory usage: 355.1 KB\n",
            "\n",
            "----- EDA Data (Wrist) -----\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 1701 entries, 0 to 1700\n",
            "Data columns (total 1 columns):\n",
            " #   Column  Non-Null Count  Dtype  \n",
            "---  ------  --------------  -----  \n",
            " 0   EDA     1701 non-null   float64\n",
            "dtypes: float64(1)\n",
            "memory usage: 13.4 KB\n",
            "\n",
            "----- TEMP Data (Wrist) -----\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 183 entries, 0 to 182\n",
            "Data columns (total 1 columns):\n",
            " #   Column  Non-Null Count  Dtype  \n",
            "---  ------  --------------  -----  \n",
            " 0   TEMP    183 non-null    float64\n",
            "dtypes: float64(1)\n",
            "memory usage: 1.6 KB\n",
            "\n",
            "--- Checking Data Types for Questionnaire Data (df_s2_quest) ---\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 23 entries, 0 to 22\n",
            "Data columns (total 1 columns):\n",
            " #   Column                              Non-Null Count  Dtype \n",
            "---  ------                              --------------  ----- \n",
            " 0   # Subj;S2;;;;;;;;;;;;;;;;;;;;;;;;;  23 non-null     object\n",
            "dtypes: object(1)\n",
            "memory usage: 316.0+ bytes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d4b97275"
      },
      "source": [
        "## Final Task\n",
        "\n",
        "### Subtask:\n",
        "Summarize the data cleaning steps performed and prepare the cleaned data for further analysis or modeling.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3850aa77"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "\n",
        "*   **Data Structure Identification**: The WESAD dataset is organized into subject-specific subdirectories (e.g., `S2/`), each containing a `.pkl` file for processed sensor data, a `_quest.csv` file for questionnaire data, and a `_E4_Data.zip` file for raw data.\n",
        "*   **Data Loading Strategy**: Due to varying sampling rates and data structures within the `S2.pkl` file, individual pandas DataFrames were created for each chest and wrist sensor (e.g., `ACC`, `ECG`, `BVP`, `EDA`, `Temp`). Accelerometer data was specifically handled to separate its x, y, and z components into distinct columns. The `S2_quest.csv` file was loaded into a separate DataFrame.\n",
        "*   **Sensor Data Consistency**:\n",
        "    *   **Chest Sensors**: All chest sensor DataFrames (`ACC`, `ECG`, `EMG`, `EDA`, `Temp`, `Resp`) have a consistent length of 4,255,300 entries, indicating uniform sampling and duration. Their data types are primarily `float64`, with `Temp` being `float32`.\n",
        "    *   **Wrist Sensors**: Wrist sensor DataFrames show varying lengths, suggesting different sampling rates or recording durations (e.g., `ACC` with 194,528 entries, `BVP` with 389,056 entries, `EDA` and `TEMP` with 24,316 entries). All wrist sensor data are `float64`.\n",
        "*   **Missing Values**: No missing values were found in any of the chest or wrist sensor DataFrames.\n",
        "*   **Duplicate Rows**: A significant number of duplicate rows were initially identified across all sensor DataFrames:\n",
        "    *   Chest sensors: Ranged from 470,641 (ACC) to 4,251,080 (Temp).\n",
        "    *   Wrist sensors: Ranged from 22,615 (EDA) to 343,616 (BVP).\n",
        "    *   All identified duplicate rows were successfully removed, resulting in zero duplicates in the cleaned sensor DataFrames.\n",
        "*   **Questionnaire Data (`df_s2_quest`) State**: The `df_s2_quest` DataFrame was loaded as a single `object` type column. It is not yet in a tabular format, requiring further parsing to extract meaningful questionnaire data and to perform checks for missing values or duplicates within its content.\n",
        "\n",
        "### Insights or Next Steps\n",
        "\n",
        "*   **Parse Questionnaire Data**: The `df_s2_quest` DataFrame needs to be parsed and transformed into a structured tabular format to make its contents accessible for analysis and to properly assess its data quality (e.g., missing values, data types).\n",
        "*   **Synchronize Sensor Data**: Given the different sampling rates and lengths of the wrist sensor DataFrames (and potentially between chest and wrist), future steps should focus on synchronizing these time-series datasets, possibly through resampling or alignment, to enable integrated analysis.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cab14871"
      },
      "source": [
        "## Save Cleaned Data\n",
        "\n",
        "### Subtask:\n",
        "Save the cleaned `df_event_timings`, `df_questionnaire_responses`, and the individual sensor DataFrames (from `chest_dataframes` and `wrist_dataframes`) to CSV files for future use. This will involve creating a new directory for cleaned data and then exporting each DataFrame."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9742999c"
      },
      "source": [
        "**Reasoning**:\n",
        "To save the cleaned data, I will first define a path for a new 'cleaned_data' directory within the `S2` subject folder. I will then create this directory if it doesn't already exist. Afterward, I will iterate through the `chest_dataframes` and `wrist_dataframes` dictionaries, saving each sensor DataFrame as a CSV file. Finally, I will save `df_event_timings` and `df_questionnaire_responses` as separate CSV files to the same directory, ensuring all processed data is persistently stored."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228
        },
        "id": "875bf236",
        "outputId": "510307b9-fa6e-49df-975f-7cef4579a06d"
      },
      "source": [
        "import os\n",
        "\n",
        "# Define the directory to save cleaned data\n",
        "cleaned_data_dir = f'{s2_data_path}/cleaned_data'\n",
        "\n",
        "# Create the directory if it does not exist\n",
        "os.makedirs(cleaned_data_dir, exist_ok=True)\n",
        "\n",
        "print(f\"Saving cleaned data to: {cleaned_data_dir}\")\n",
        "\n",
        "# Save df_event_timings\n",
        "df_event_timings.to_csv(f'{cleaned_data_dir}/df_event_timings.csv', index=False)\n",
        "print(\"Saved df_event_timings.csv\")\n",
        "\n",
        "# Save df_questionnaire_responses\n",
        "df_questionnaire_responses.to_csv(f'{cleaned_data_dir}/df_questionnaire_responses.csv', index=False)\n",
        "print(\"Saved df_questionnaire_responses.csv\")\n",
        "\n",
        "# Save individual chest sensor DataFrames\n",
        "for sensor_name, df in chest_dataframes.items():\n",
        "    df.to_csv(f'{cleaned_data_dir}/chest_{sensor_name.lower()}.csv', index=False)\n",
        "    print(f\"Saved chest_{sensor_name.lower()}.csv\")\n",
        "\n",
        "# Save individual wrist sensor DataFrames\n",
        "for sensor_name, df in wrist_dataframes.items():\n",
        "    df.to_csv(f'{cleaned_data_dir}/wrist_{sensor_name.lower()}.csv', index=False)\n",
        "    print(f\"Saved wrist_{sensor_name.lower()}.csv\")\n",
        "\n",
        "print(\"All cleaned DataFrames saved successfully!\")"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving cleaned data to: /content/WESAD_data/WESAD/S2/cleaned_data\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'df_event_timings' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3577256312.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# Save df_event_timings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mdf_event_timings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'{cleaned_data_dir}/df_event_timings.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Saved df_event_timings.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'df_event_timings' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9e0dbd82"
      },
      "source": [
        "# Task\n",
        "Inspect the `df_s2_quest` DataFrame to understand its structure for parsing, including printing additional rows and specific slices to identify patterns in the header information and data rows."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8d3745c3"
      },
      "source": [
        "## Inspect df_s2_quest structure\n",
        "\n",
        "### Subtask:\n",
        "Further inspect the `df_s2_quest` to understand its exact structure and identify patterns for parsing, especially header information and data rows. This may involve printing more rows or specific slices of the DataFrame.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7e9951ae"
      },
      "source": [
        "**Reasoning**:\n",
        "To understand the structure of `df_s2_quest` for parsing, I will display all its rows, then specifically print the first row and a few other distinct rows to identify patterns in how different types of information are presented.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ace49612",
        "outputId": "9ac85245-03aa-44fa-a2b5-c43fe500137f"
      },
      "source": [
        "print(\"--- Displaying all rows of df_s2_quest ---\")\n",
        "print(df_s2_quest.to_string())\n",
        "\n",
        "print(\"\\n--- First row of df_s2_quest ---\")\n",
        "print(df_s2_quest.iloc[0])\n",
        "\n",
        "print(\"\\n--- Second row of df_s2_quest ---\")\n",
        "print(df_s2_quest.iloc[1])\n",
        "\n",
        "print(\"\\n--- Third row of df_s2_quest ---\")\n",
        "print(df_s2_quest.iloc[2])\n",
        "\n",
        "print(\"\\n--- Fifth row of df_s2_quest (index 4) ---\")\n",
        "print(df_s2_quest.iloc[4])"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Displaying all rows of df_s2_quest ---\n",
            "                                     # Subj;S2;;;;;;;;;;;;;;;;;;;;;;;;;\n",
            "0    # ORDER;Base;TSST;Medi 1;Fun;Medi 2;sRead;fRead;;;;;;;;;;;;;;;;;;;\n",
            "1   # START;7.08;39.55;70.19;81.25;93.38;54.42;89.51;;;;;;;;;;;;;;;;;;;\n",
            "2     # END;26.32;50.3;77.1;87.47;100.15;56.07;91.15;;;;;;;;;;;;;;;;;;;\n",
            "3                                            ;;;;;;;;;;;;;;;;;;;;;;;;;;\n",
            "4             # PANAS;1;1;3;2;1;3;1;1;1;2;2;2;2;1;4;3;4;4;2;2;2;1;2;1;;\n",
            "5           # PANAS;3;2;4;1;3;3;1;2;1;4;2;4;3;1;5;4;4;4;2;3;3;3;2;1;3;1\n",
            "6             # PANAS;1;1;2;3;1;2;1;1;1;1;1;1;3;1;2;1;2;3;1;1;1;1;4;1;;\n",
            "7             # PANAS;1;1;2;3;1;1;1;1;1;1;1;1;2;1;4;1;1;3;1;1;1;2;3;1;;\n",
            "8             # PANAS;1;1;1;2;1;1;1;1;1;1;1;1;2;1;2;1;1;2;1;1;1;1;3;1;;\n",
            "9                                            ;;;;;;;;;;;;;;;;;;;;;;;;;;\n",
            "10                               # STAI;3;2;1;4;2;3;;;;;;;;;;;;;;;;;;;;\n",
            "11                               # STAI;1;3;2;1;3;1;;;;;;;;;;;;;;;;;;;;\n",
            "12                               # STAI;4;1;1;4;2;4;;;;;;;;;;;;;;;;;;;;\n",
            "13                               # STAI;3;1;1;3;1;4;;;;;;;;;;;;;;;;;;;;\n",
            "14                               # STAI;4;1;1;4;1;4;;;;;;;;;;;;;;;;;;;;\n",
            "15                                           ;;;;;;;;;;;;;;;;;;;;;;;;;;\n",
            "16                                    # DIM;7;2;;;;;;;;;;;;;;;;;;;;;;;;\n",
            "17                                    # DIM;5;4;;;;;;;;;;;;;;;;;;;;;;;;\n",
            "18                                    # DIM;7;2;;;;;;;;;;;;;;;;;;;;;;;;\n",
            "19                                    # DIM;8;1;;;;;;;;;;;;;;;;;;;;;;;;\n",
            "20                                    # DIM;7;2;;;;;;;;;;;;;;;;;;;;;;;;\n",
            "21                                           ;;;;;;;;;;;;;;;;;;;;;;;;;;\n",
            "22                               # SSSQ;5;5;4;4;3;5;;;;;;;;;;;;;;;;;;;;\n",
            "\n",
            "--- First row of df_s2_quest ---\n",
            "# Subj;S2;;;;;;;;;;;;;;;;;;;;;;;;;    # ORDER;Base;TSST;Medi 1;Fun;Medi 2;sRead;fRea...\n",
            "Name: 0, dtype: object\n",
            "\n",
            "--- Second row of df_s2_quest ---\n",
            "# Subj;S2;;;;;;;;;;;;;;;;;;;;;;;;;    # START;7.08;39.55;70.19;81.25;93.38;54.42;89....\n",
            "Name: 1, dtype: object\n",
            "\n",
            "--- Third row of df_s2_quest ---\n",
            "# Subj;S2;;;;;;;;;;;;;;;;;;;;;;;;;    # END;26.32;50.3;77.1;87.47;100.15;56.07;91.15...\n",
            "Name: 2, dtype: object\n",
            "\n",
            "--- Fifth row of df_s2_quest (index 4) ---\n",
            "# Subj;S2;;;;;;;;;;;;;;;;;;;;;;;;;    # PANAS;1;1;3;2;1;3;1;1;1;2;2;2;2;1;4;3;4;4;2;...\n",
            "Name: 4, dtype: object\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0d715bcd"
      },
      "source": [
        "The inspection of `df_s2_quest` has provided a clear understanding of its structure:\n",
        "\n",
        "*   **Single Column Format**: The entire CSV content is loaded into a single column, with each row being a string containing semicolon-separated values.\n",
        "*   **Header Rows**: The first few rows (`#ORDER`, `#START`, `#END`) provide metadata about the experimental setup (e.g., order of tasks, start and end times). These rows contain both text labels and numeric values.\n",
        "*   **Questionnaire Data Rows**: Subsequent rows (`#PANAS`, `#STAI`, `#DIM`, `#SSSQ`) contain responses to different questionnaires. Each of these lines starts with a tag (e.g., `#PANAS`) followed by a series of numeric responses, also semicolon-separated.\n",
        "*   **Empty Rows**: There are rows containing only semicolons (`;;;;...`) which act as separators between different sections of the questionnaire data.\n",
        "*   **Delimiter**: The primary delimiter for all values appears to be a semicolon (`;`).\n",
        "*   **Subject Identifier**: The very first line `\"# Subj;S2;;;;;;;;;;;;;;;;;;;;;;;;;\"` identifies the subject. This information was implicitly captured when loading the `S2_quest.csv` file.\n",
        "\n",
        "**Next Steps for Parsing `df_s2_quest`:**\n",
        "\n",
        "1.  **Extract Metadata**: Parse the `#ORDER`, `#START`, and `#END` rows to extract the task sequence and their corresponding start/end times. This data could be stored in a dictionary or a separate DataFrame.\n",
        "2.  **Extract Questionnaire Data**: Iterate through the remaining rows, identify each questionnaire type (PANAS, STAI, DIM, SSSQ), and extract the numerical responses into separate pandas DataFrames, possibly with appropriate column names derived from the questionnaire structure (if available in documentation or inferable).\n",
        "3.  **Clean and Convert Data Types**: Ensure extracted numerical data is converted to appropriate numeric types (e.g., `int` or `float`).\n",
        "4.  **Handle Empty/Irrelevant Values**: Remove or appropriately handle the trailing empty fields (`;;`) and the empty separator rows.\n",
        "\n",
        "This detailed understanding of the `df_s2_quest` structure will guide the next data cleaning steps to transform this raw questionnaire data into a usable tabular format."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "29f1e7c2"
      },
      "source": [
        "## Extract event timings\n",
        "\n",
        "### Subtask:\n",
        "Parse the 'START' and 'END' rows from `df_s2_quest` to extract the event timings for different experimental phases (e.g., Base, TSST, Medi 1, etc.) and store them in a suitable data structure, like a dictionary or a small DataFrame.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "59e8f17d"
      },
      "source": [
        "**Reasoning**:\n",
        "To extract event timings, I will locate the '# ORDER', '# START', and '# END' rows, split their string content by semicolons, clean the resulting lists by removing empty strings and the initial '#' character, create event names from the '# ORDER' row, and then construct dictionaries for 'START' and 'END' times. Finally, I will combine these into a new pandas DataFrame as instructed.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4f76da6b",
        "outputId": "be51494c-d1ad-48b7-bcc2-82521b4fac4b"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "# 1. Locate and extract the string content from the relevant rows\n",
        "order_str = df_s2_quest.iloc[0, 0]\n",
        "start_str = df_s2_quest.iloc[1, 0]\n",
        "end_str = df_s2_quest.iloc[2, 0]\n",
        "\n",
        "# 2. Split by semicolon and clean the lists\n",
        "# Remove empty strings and the initial '#' from the first element\n",
        "\n",
        "def clean_split_list(s):\n",
        "    # Remove the first '#' if present and split\n",
        "    parts = s.replace('#', '').split(';')\n",
        "    # Filter out empty strings\n",
        "    return [p.strip() for p in parts if p.strip()]\n",
        "\n",
        "cleaned_order = clean_split_list(order_str)\n",
        "cleaned_start = clean_split_list(start_str)\n",
        "cleaned_end = clean_split_list(end_str)\n",
        "\n",
        "# 3. Create a list of event names (skipping the 'ORDER' label)\n",
        "event_names = cleaned_order[1:]\n",
        "\n",
        "# 4. Create dictionaries for START and END times, converting to float\n",
        "start_times = {}\n",
        "for i, event in enumerate(event_names):\n",
        "    # Ensure index is within bounds for start_str data, skipping the 'START' label\n",
        "    if (i + 1) < len(cleaned_start):\n",
        "        try:\n",
        "            start_times[event] = float(cleaned_start[i + 1])\n",
        "        except ValueError:\n",
        "            start_times[event] = None # Handle cases where conversion to float fails\n",
        "\n",
        "end_times = {}\n",
        "for i, event in enumerate(event_names):\n",
        "    # Ensure index is within bounds for end_str data, skipping the 'END' label\n",
        "    if (i + 1) < len(cleaned_end):\n",
        "        try:\n",
        "            end_times[event] = float(cleaned_end[i + 1])\n",
        "        except ValueError:\n",
        "            end_times[event] = None # Handle cases where conversion to float fails\n",
        "\n",
        "# 5. Combine into a new pandas DataFrame\n",
        "# Create lists for DataFrame construction\n",
        "events_list = []\n",
        "start_time_list = []\n",
        "end_time_list = []\n",
        "\n",
        "for event in event_names:\n",
        "    events_list.append(event)\n",
        "    start_time_list.append(start_times.get(event))\n",
        "    end_time_list.append(end_times.get(event))\n",
        "\n",
        "df_event_timings = pd.DataFrame({\n",
        "    'Event': events_list,\n",
        "    'Start_Time': start_time_list,\n",
        "    'End_Time': end_time_list\n",
        "})\n",
        "\n",
        "print(\"Event Timings DataFrame:\")\n",
        "print(df_event_timings)\n"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Event Timings DataFrame:\n",
            "    Event  Start_Time  End_Time\n",
            "0    Base        7.08     26.32\n",
            "1    TSST       39.55     50.30\n",
            "2  Medi 1       70.19     77.10\n",
            "3     Fun       81.25     87.47\n",
            "4  Medi 2       93.38    100.15\n",
            "5   sRead       54.42     56.07\n",
            "6   fRead       89.51     91.15\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "99cf2a62"
      },
      "source": [
        "## Extract questionnaire responses\n",
        "\n",
        "### Subtask:\n",
        "Parse the 'PANAS', 'STAI', 'DIM', and 'SSSQ' questionnaire response rows from `df_s2_quest`.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "34449d2b"
      },
      "source": [
        "**Reasoning**:\n",
        "To parse the questionnaire responses, I will iterate through the `df_s2_quest` DataFrame, identify rows corresponding to 'PANAS', 'STAI', 'DIM', and 'SSSQ' questionnaires, extract the numeric data by splitting the strings and converting values to float, and then store these responses into separate pandas DataFrames as instructed.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b0ee8355",
        "outputId": "0e4e8514-fc09-4aa9-d401-7f7996d9450f"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Initialize empty lists to store parsed responses for each questionnaire type\n",
        "panas_data = []\n",
        "stai_data = []\n",
        "dim_data = []\n",
        "sssq_data = []\n",
        "\n",
        "# Iterate through df_s2_quest from row index 4 onwards (as metadata is already extracted)\n",
        "for i in range(4, len(df_s2_quest)):\n",
        "    row_string = df_s2_quest.iloc[i, 0]\n",
        "\n",
        "    # Check if the row starts with a questionnaire identifier and process it\n",
        "    if row_string.startswith('# PANAS'):\n",
        "        # Remove the tag and split by semicolon, then convert to numeric\n",
        "        parts = row_string.replace('# PANAS;', '').split(';')\n",
        "        numeric_parts = [float(p.strip()) for p in parts if p.strip()]\n",
        "        if numeric_parts:\n",
        "            panas_data.append(numeric_parts)\n",
        "    elif row_string.startswith('# STAI'):\n",
        "        parts = row_string.replace('# STAI;', '').split(';')\n",
        "        numeric_parts = [float(p.strip()) for p in parts if p.strip()]\n",
        "        if numeric_parts:\n",
        "            stai_data.append(numeric_parts)\n",
        "    elif row_string.startswith('# DIM'):\n",
        "        parts = row_string.replace('# DIM;', '').split(';')\n",
        "        numeric_parts = [float(p.strip()) for p in parts if p.strip()]\n",
        "        if numeric_parts:\n",
        "            dim_data.append(numeric_parts)\n",
        "    elif row_string.startswith('# SSSQ'):\n",
        "        parts = row_string.replace('# SSSQ;', '').split(';')\n",
        "        numeric_parts = [float(p.strip()) for p in parts if p.strip()]\n",
        "        if numeric_parts:\n",
        "            sssq_data.append(numeric_parts)\n",
        "\n",
        "# Convert lists of lists to pandas DataFrames. pd.DataFrame handles varying row lengths by padding with NaN.\n",
        "df_panas = pd.DataFrame(panas_data)\n",
        "df_stai = pd.DataFrame(stai_data)\n",
        "df_dim = pd.DataFrame(dim_data)\n",
        "df_sssq = pd.DataFrame(sssq_data)\n",
        "\n",
        "# Print the head of each created DataFrame to inspect the parsed questionnaire responses\n",
        "print(\"\\n--- df_panas head ---\")\n",
        "print(df_panas.head())\n",
        "\n",
        "print(\"\\n--- df_stai head ---\")\n",
        "print(df_stai.head())\n",
        "\n",
        "print(\"\\n--- df_dim head ---\")\n",
        "print(df_dim.head())\n",
        "\n",
        "print(\"\\n--- df_sssq head ---\")\n",
        "print(df_sssq.head())"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- df_panas head ---\n",
            "    0    1    2    3    4    5    6    7    8    9   ...   16   17   18   19  \\\n",
            "0  1.0  1.0  3.0  2.0  1.0  3.0  1.0  1.0  1.0  2.0  ...  4.0  4.0  2.0  2.0   \n",
            "1  3.0  2.0  4.0  1.0  3.0  3.0  1.0  2.0  1.0  4.0  ...  4.0  4.0  2.0  3.0   \n",
            "2  1.0  1.0  2.0  3.0  1.0  2.0  1.0  1.0  1.0  1.0  ...  2.0  3.0  1.0  1.0   \n",
            "3  1.0  1.0  2.0  3.0  1.0  1.0  1.0  1.0  1.0  1.0  ...  1.0  3.0  1.0  1.0   \n",
            "4  1.0  1.0  1.0  2.0  1.0  1.0  1.0  1.0  1.0  1.0  ...  1.0  2.0  1.0  1.0   \n",
            "\n",
            "    20   21   22   23   24   25  \n",
            "0  2.0  1.0  2.0  1.0  NaN  NaN  \n",
            "1  3.0  3.0  2.0  1.0  3.0  1.0  \n",
            "2  1.0  1.0  4.0  1.0  NaN  NaN  \n",
            "3  1.0  2.0  3.0  1.0  NaN  NaN  \n",
            "4  1.0  1.0  3.0  1.0  NaN  NaN  \n",
            "\n",
            "[5 rows x 26 columns]\n",
            "\n",
            "--- df_stai head ---\n",
            "     0    1    2    3    4    5\n",
            "0  3.0  2.0  1.0  4.0  2.0  3.0\n",
            "1  1.0  3.0  2.0  1.0  3.0  1.0\n",
            "2  4.0  1.0  1.0  4.0  2.0  4.0\n",
            "3  3.0  1.0  1.0  3.0  1.0  4.0\n",
            "4  4.0  1.0  1.0  4.0  1.0  4.0\n",
            "\n",
            "--- df_dim head ---\n",
            "     0    1\n",
            "0  7.0  2.0\n",
            "1  5.0  4.0\n",
            "2  7.0  2.0\n",
            "3  8.0  1.0\n",
            "4  7.0  2.0\n",
            "\n",
            "--- df_sssq head ---\n",
            "     0    1    2    3    4    5\n",
            "0  5.0  5.0  4.0  4.0  3.0  5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c847b950"
      },
      "source": [
        "## Create structured DataFrame\n",
        "\n",
        "### Subtask:\n",
        "Combine the extracted questionnaire responses into a single, comprehensive pandas DataFrame, ensuring each question's response is in its own column and rows represent individual entries or participants, as appropriate.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "135f5193"
      },
      "source": [
        "**Reasoning**:\n",
        "To combine the extracted questionnaire responses into a single DataFrame as instructed, I will first rename the columns of each individual questionnaire DataFrame with a descriptive prefix. Then, I will horizontally concatenate 'df_panas', 'df_stai', and 'df_dim', extend 'df_sssq' to match the row count, and finally concatenate it to the combined DataFrame, before printing the head and info of the resulting 'df_questionnaire_responses'.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a24052b9",
        "outputId": "b7e9bda7-dd46-46de-f5f4-5290829932ab"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "# 1. Rename the columns of each DataFrame\n",
        "df_panas.columns = [f'PANAS_{i}' for i in range(len(df_panas.columns))]\n",
        "df_stai.columns = [f'STAI_{i}' for i in range(len(df_stai.columns))]\n",
        "df_dim.columns = [f'DIM_{i}' for i in range(len(df_dim.columns))]\n",
        "df_sssq.columns = [f'SSSQ_{i}' for i in range(len(df_sssq.columns))]\n",
        "\n",
        "# 2. Concatenate df_panas, df_stai, and df_dim horizontally\n",
        "df_combined_responses = pd.concat([df_panas, df_stai, df_dim], axis=1)\n",
        "\n",
        "# 3. Create a new DataFrame for df_sssq with the same number of rows as df_combined_responses\n",
        "# Get the number of rows from df_combined_responses\n",
        "num_rows = len(df_combined_responses)\n",
        "\n",
        "# Create an empty DataFrame with the correct number of rows and columns from df_sssq\n",
        "df_sssq_extended = pd.DataFrame(np.nan, index=range(num_rows), columns=df_sssq.columns)\n",
        "\n",
        "# Place the actual df_sssq data (which is a single row) into the first row of the extended DataFrame\n",
        "if not df_sssq.empty:\n",
        "    df_sssq_extended.iloc[0] = df_sssq.iloc[0]\n",
        "\n",
        "# 4. Horizontally concatenate the extended df_sssq DataFrame with df_combined_responses\n",
        "df_questionnaire_responses = pd.concat([df_combined_responses, df_sssq_extended], axis=1)\n",
        "\n",
        "# 5. Print the head and information of the final df_questionnaire_responses DataFrame\n",
        "print(\"--- df_questionnaire_responses head ---\")\n",
        "print(df_questionnaire_responses.head())\n",
        "\n",
        "print(\"\\n--- df_questionnaire_responses info ---\")\n",
        "df_questionnaire_responses.info()"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- df_questionnaire_responses head ---\n",
            "   PANAS_0  PANAS_1  PANAS_2  PANAS_3  PANAS_4  PANAS_5  PANAS_6  PANAS_7  \\\n",
            "0      1.0      1.0      3.0      2.0      1.0      3.0      1.0      1.0   \n",
            "1      3.0      2.0      4.0      1.0      3.0      3.0      1.0      2.0   \n",
            "2      1.0      1.0      2.0      3.0      1.0      2.0      1.0      1.0   \n",
            "3      1.0      1.0      2.0      3.0      1.0      1.0      1.0      1.0   \n",
            "4      1.0      1.0      1.0      2.0      1.0      1.0      1.0      1.0   \n",
            "\n",
            "   PANAS_8  PANAS_9  ...  STAI_4  STAI_5  DIM_0  DIM_1  SSSQ_0  SSSQ_1  \\\n",
            "0      1.0      2.0  ...     2.0     3.0    7.0    2.0     5.0     5.0   \n",
            "1      1.0      4.0  ...     3.0     1.0    5.0    4.0     NaN     NaN   \n",
            "2      1.0      1.0  ...     2.0     4.0    7.0    2.0     NaN     NaN   \n",
            "3      1.0      1.0  ...     1.0     4.0    8.0    1.0     NaN     NaN   \n",
            "4      1.0      1.0  ...     1.0     4.0    7.0    2.0     NaN     NaN   \n",
            "\n",
            "   SSSQ_2  SSSQ_3  SSSQ_4  SSSQ_5  \n",
            "0     4.0     4.0     3.0     5.0  \n",
            "1     NaN     NaN     NaN     NaN  \n",
            "2     NaN     NaN     NaN     NaN  \n",
            "3     NaN     NaN     NaN     NaN  \n",
            "4     NaN     NaN     NaN     NaN  \n",
            "\n",
            "[5 rows x 40 columns]\n",
            "\n",
            "--- df_questionnaire_responses info ---\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 5 entries, 0 to 4\n",
            "Data columns (total 40 columns):\n",
            " #   Column    Non-Null Count  Dtype  \n",
            "---  ------    --------------  -----  \n",
            " 0   PANAS_0   5 non-null      float64\n",
            " 1   PANAS_1   5 non-null      float64\n",
            " 2   PANAS_2   5 non-null      float64\n",
            " 3   PANAS_3   5 non-null      float64\n",
            " 4   PANAS_4   5 non-null      float64\n",
            " 5   PANAS_5   5 non-null      float64\n",
            " 6   PANAS_6   5 non-null      float64\n",
            " 7   PANAS_7   5 non-null      float64\n",
            " 8   PANAS_8   5 non-null      float64\n",
            " 9   PANAS_9   5 non-null      float64\n",
            " 10  PANAS_10  5 non-null      float64\n",
            " 11  PANAS_11  5 non-null      float64\n",
            " 12  PANAS_12  5 non-null      float64\n",
            " 13  PANAS_13  5 non-null      float64\n",
            " 14  PANAS_14  5 non-null      float64\n",
            " 15  PANAS_15  5 non-null      float64\n",
            " 16  PANAS_16  5 non-null      float64\n",
            " 17  PANAS_17  5 non-null      float64\n",
            " 18  PANAS_18  5 non-null      float64\n",
            " 19  PANAS_19  5 non-null      float64\n",
            " 20  PANAS_20  5 non-null      float64\n",
            " 21  PANAS_21  5 non-null      float64\n",
            " 22  PANAS_22  5 non-null      float64\n",
            " 23  PANAS_23  5 non-null      float64\n",
            " 24  PANAS_24  1 non-null      float64\n",
            " 25  PANAS_25  1 non-null      float64\n",
            " 26  STAI_0    5 non-null      float64\n",
            " 27  STAI_1    5 non-null      float64\n",
            " 28  STAI_2    5 non-null      float64\n",
            " 29  STAI_3    5 non-null      float64\n",
            " 30  STAI_4    5 non-null      float64\n",
            " 31  STAI_5    5 non-null      float64\n",
            " 32  DIM_0     5 non-null      float64\n",
            " 33  DIM_1     5 non-null      float64\n",
            " 34  SSSQ_0    1 non-null      float64\n",
            " 35  SSSQ_1    1 non-null      float64\n",
            " 36  SSSQ_2    1 non-null      float64\n",
            " 37  SSSQ_3    1 non-null      float64\n",
            " 38  SSSQ_4    1 non-null      float64\n",
            " 39  SSSQ_5    1 non-null      float64\n",
            "dtypes: float64(40)\n",
            "memory usage: 1.7 KB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0e0f466c"
      },
      "source": [
        "## Final Task\n",
        "\n",
        "### Subtask:\n",
        "Summarize the parsing process and the resulting structured DataFrame, including a preview of the cleaned questionnaire data and a brief discussion of its readiness for further analysis.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "49c2db2c"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "\n",
        "*   The initial `df_s2_quest` DataFrame was identified as a single-column structure where each row contained semicolon-separated values.\n",
        "*   The raw data contained distinct sections: header information (`#ORDER`, `#START`, `#END`), questionnaire responses (`#PANAS`, `#STAI`, `#DIM`, `#SSSQ`), and empty separator rows.\n",
        "*   Event timings were successfully extracted from the `#ORDER`, `#START`, and `#END` rows into a `df_event_timings` DataFrame. For instance, the 'Base' event has a Start\\_Time of 7.08 and an End\\_Time of 26.32.\n",
        "*   Individual questionnaire responses were parsed into separate DataFrames: `df_panas` (26 columns), `df_stai` (6 columns), `df_dim` (2 columns), and `df_sssq` (6 columns), handling varying response lengths with `NaN` values where necessary.\n",
        "*   The final combined `df_questionnaire_responses` DataFrame consists of 5 entries and 40 columns, with all data converted to `float64`. Columns were systematically renamed (e.g., `PANAS_0`, `STAI_0`), and `df_sssq` (originally a single row) was appropriately extended with `NaN` values to match the row count of other questionnaires during concatenation.\n",
        "\n",
        "### Insights or Next Steps\n",
        "\n",
        "*   The questionnaire data is now in a clean, structured format, making it ready for statistical analysis, such as calculating questionnaire scores, conducting correlation analysis with experimental events, or preparing for machine learning models.\n",
        "*   Further analysis could involve integrating `df_questionnaire_responses` with `df_event_timings` and other subject-specific data to build a holistic dataset for in-depth insights into experimental outcomes and participant responses.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "28b307a4"
      },
      "source": [
        "## Verify Saved Files\n",
        "\n",
        "### Subtask:\n",
        "List the contents of the `cleaned_data` directory to confirm that all DataFrames were successfully saved as CSV files."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d8bfd42e"
      },
      "source": [
        "**Reasoning**:\n",
        "To verify that the files have been saved, I will use the `!ls` command to list the contents of the `cleaned_data_dir`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "31e587a6",
        "outputId": "f3b49a18-6c21-40f6-a367-cadb7f4f6940"
      },
      "source": [
        "print(f\"Listing files in {cleaned_data_dir}:\")\n",
        "!ls -F {cleaned_data_dir}"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Listing files in /content/WESAD_data/WESAD/S2/cleaned_data:\n"
          ]
        }
      ]
    }
  ]
}