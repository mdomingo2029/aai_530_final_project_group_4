{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPiQftcha0pkql2bWMG6Uvv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mdomingo2029/aai_530_final_project_group_4/blob/main/ai_data_cleaning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KtEx43QTbEJ2"
      },
      "outputs": [],
      "source": [
        "!pip install ucimlrepo"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "H80cVKV3bvUR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls \"/content/drive/MyDrive/WESAD.zip\""
      ],
      "metadata": {
        "id": "ni1oarDfbvL9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "66158b95"
      },
      "source": [
        "print('Listing contents of MyDrive:')\n",
        "!ls \"/content/drive/MyDrive\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0afa312a"
      },
      "source": [
        "If your `WESAD.zip` file is inside a subfolder, for example, named `MyProject`, you would then run:\n",
        "\n",
        "```python\n",
        "!ls \"/content/drive/MyDrive/MyProject\"\n",
        "```\n",
        "\n",
        "Once you find the file, use the full path in your code."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "\n",
        "# 1. Mount the drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# 2. Define the exact path (Google Drive root is 'MyDrive')\n",
        "zip_path = '/content/drive/MyDrive/WESAD.zip'\n",
        "\n",
        "# 3. Check if the file exists before unzipping\n",
        "if os.path.exists(zip_path):\n",
        "    print(\"Found it! Unzipping now...\")\n",
        "    !unzip -q \"{zip_path}\" -d \"/content/WESAD_data\"\n",
        "    print(\"Done! Files are now in the 'WESAD_data' folder on the left sidebar.\")\n",
        "else:\n",
        "    print(\"Still can't see it. Make sure the file is in the main 'My Drive' folder, not a subfolder.\")"
      ],
      "metadata": {
        "id": "yH0pkVivmCPM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aa8abf94"
      },
      "source": [
        "# Task\n",
        "Explore the structure of the unzipped WESAD data by listing the contents of the `/content/WESAD_data` directory to identify the main data files."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9a124f70"
      },
      "source": [
        "## Explore Unzipped Data\n",
        "\n",
        "### Subtask:\n",
        "Explore the structure of the unzipped WESAD data by listing the contents of the `/content/WESAD_data` directory to identify the main data files.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a59b22cf"
      },
      "source": [
        "**Reasoning**:\n",
        "To explore the structure of the unzipped WESAD data, I will list the contents of the `/content/WESAD_data` directory using the `!ls -F` command as instructed.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ebd61aa7"
      },
      "source": [
        "print('Listing contents of the unzipped WESAD data directory:')\n",
        "!ls -F /content/WESAD_data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ba54179c"
      },
      "source": [
        "**Reasoning**:\n",
        "Since the previous `ls` command revealed a subdirectory named `WESAD/`, I will now list the contents of this subdirectory to further explore the data structure and locate the actual data files.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cbb4aa9e"
      },
      "source": [
        "print('Listing contents of the WESAD subdirectory:')\n",
        "!ls -F /content/WESAD_data/WESAD"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d6a5b1e7"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous command revealed subject-specific subdirectories (S2/, S3/, etc.) and a `wesad_readme.pdf` file. To understand the data structure further, I will list the contents of one of these subject directories, specifically `S2/`, to see what kind of data files are stored there.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "635b6015"
      },
      "source": [
        "print('Listing contents of the S2 subdirectory:')\n",
        "!ls -F /content/WESAD_data/WESAD/S2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e963a801"
      },
      "source": [
        "## Load Data\n",
        "\n",
        "### Subtask:\n",
        "Load the relevant WESAD data files into pandas DataFrames for cleaning.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "94ad475b"
      },
      "source": [
        "**Reasoning**:\n",
        "I will import the pandas library, define the base path to the S2 subject data, and then load the 'S2.pkl' and 'S2_quest.csv' files into two separate pandas DataFrames, `df_s2_data` and `df_s2_quest` respectively, to begin the data loading process as instructed.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b274afb1"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Define the base path to the S2 subject data directory\n",
        "s2_data_path = '/content/WESAD_data/WESAD/S2'\n",
        "\n",
        "# Load the S2.pkl file into a pandas DataFrame\n",
        "df_s2_data = pd.read_pickle(f'{s2_data_path}/S2.pkl')\n",
        "\n",
        "# Load the S2_quest.csv file into a pandas DataFrame\n",
        "df_s2_quest = pd.read_csv(f'{s2_data_path}/S2_quest.csv')\n",
        "\n",
        "print(\"df_s2_data head:\")\n",
        "print(df_s2_data.head())\n",
        "\n",
        "print(\"\\ndf_s2_quest head:\")\n",
        "print(df_s2_quest.head())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c34221d1"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous code failed because `df_s2_data` was a dictionary, not a pandas DataFrame, and therefore did not have a `.head()` method. The `S2.pkl` file, when loaded, is a dictionary containing various data points. To address the subtask of loading relevant data into DataFrames, I will first load the `.pkl` file into a dictionary, then extract the 'chest' and 'wrist' signal data from it and convert them into separate pandas DataFrames. I will also correctly load the `S2_quest.csv` into a DataFrame.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2d85dabe"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Define the base path to the S2 subject data directory\n",
        "s2_data_path = '/content/WESAD_data/WESAD/S2'\n",
        "\n",
        "# Load the S2.pkl file into a dictionary first\n",
        "s2_raw_data_dict = pd.read_pickle(f'{s2_data_path}/S2.pkl')\n",
        "\n",
        "# Extract 'chest' and 'wrist' signals and convert them to DataFrames\n",
        "df_s2_chest_data = pd.DataFrame(s2_raw_data_dict['signal']['chest'])\n",
        "df_s2_wrist_data = pd.DataFrame(s2_raw_data_dict['signal']['wrist'])\n",
        "\n",
        "# Load the S2_quest.csv file into a pandas DataFrame\n",
        "df_s2_quest = pd.read_csv(f'{s2_data_path}/S2_quest.csv')\n",
        "\n",
        "print(\"df_s2_chest_data head:\")\n",
        "print(df_s2_chest_data.head())\n",
        "\n",
        "print(\"\\ndf_s2_wrist_data head:\")\n",
        "print(df_s2_wrist_data.head())\n",
        "\n",
        "print(\"\\ndf_s2_quest head:\")\n",
        "print(df_s2_quest.head())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3222e2e5"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous code failed because the 'ACC' (accelerometer) data within the 'chest' and 'wrist' signals of the `S2.pkl` file are multi-dimensional arrays, causing a `ValueError` when `pd.DataFrame` tried to interpret them as single columns. To fix this, I need to explicitly extract the 'ACC' data into separate columns (e.g., 'ACC_x', 'ACC_y', 'ACC_z') and then create DataFrames from all the extracted 1-dimensional signal arrays.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8035ff91"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Define the base path to the S2 subject data directory\n",
        "s2_data_path = '/content/WESAD_data/WESAD/S2'\n",
        "\n",
        "# Load the S2.pkl file into a dictionary first\n",
        "s2_raw_data_dict = pd.read_pickle(f'{s2_data_path}/S2.pkl')\n",
        "\n",
        "# Process 'chest' signals\n",
        "chest_signals = s2_raw_data_dict['signal']['chest']\n",
        "chest_data = {}\n",
        "for sensor_name, data_array in chest_signals.items():\n",
        "    if sensor_name == 'ACC':\n",
        "        # Accelerometer data is typically 3-axis, so create three columns\n",
        "        chest_data['ACC_x'] = data_array[:, 0]\n",
        "        chest_data['ACC_y'] = data_array[:, 1]\n",
        "        chest_data['ACC_z'] = data_array[:, 2]\n",
        "    else:\n",
        "        # Other sensors are typically 1D\n",
        "        chest_data[sensor_name] = data_array\n",
        "\n",
        "df_s2_chest_data = pd.DataFrame(chest_data)\n",
        "\n",
        "# Process 'wrist' signals\n",
        "wrist_signals = s2_raw_data_dict['signal']['wrist']\n",
        "wrist_data = {}\n",
        "for sensor_name, data_array in wrist_signals.items():\n",
        "    if sensor_name == 'ACC':\n",
        "        # Accelerometer data is typically 3-axis, so create three columns\n",
        "        wrist_data['ACC_x'] = data_array[:, 0]\n",
        "        wrist_data['ACC_y'] = data_array[:, 1]\n",
        "        wrist_data['ACC_z'] = data_array[:, 2]\n",
        "    else:\n",
        "        # Other sensors are typically 1D\n",
        "        wrist_data[sensor_name] = data_array\n",
        "\n",
        "df_s2_wrist_data = pd.DataFrame(wrist_data)\n",
        "\n",
        "# Load the S2_quest.csv file into a pandas DataFrame\n",
        "df_s2_quest = pd.read_csv(f'{s2_data_path}/S2_quest.csv')\n",
        "\n",
        "print(\"df_s2_chest_data head:\")\n",
        "print(df_s2_chest_data.head())\n",
        "\n",
        "print(\"\\ndf_s2_wrist_data head:\")\n",
        "print(df_s2_wrist_data.head())\n",
        "\n",
        "print(\"\\ndf_s2_quest head:\")\n",
        "print(df_s2_quest.head())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8636bc75"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous code failed because some non-accelerometer sensor data (e.g., ECG, EMG) were still 2-dimensional arrays with a single column, which `pd.DataFrame` cannot directly handle as individual columns. I need to flatten these 2D arrays into 1-dimensional arrays before adding them to the `chest_data` and `wrist_data` dictionaries to resolve the `ValueError`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d2b017ad"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Define the base path to the S2 subject data directory\n",
        "s2_data_path = '/content/WESAD_data/WESAD/S2'\n",
        "\n",
        "# Load the S2.pkl file into a dictionary first\n",
        "s2_raw_data_dict = pd.read_pickle(f'{s2_data_path}/S2.pkl')\n",
        "\n",
        "# Process 'chest' signals\n",
        "chest_signals = s2_raw_data_dict['signal']['chest']\n",
        "chest_data = {}\n",
        "for sensor_name, data_array in chest_signals.items():\n",
        "    if sensor_name == 'ACC':\n",
        "        # Accelerometer data is typically 3-axis, so create three columns\n",
        "        chest_data['ACC_x'] = data_array[:, 0]\n",
        "        chest_data['ACC_y'] = data_array[:, 1]\n",
        "        chest_data['ACC_z'] = data_array[:, 2]\n",
        "    else:\n",
        "        # Other sensors might be 2D arrays with a single column, flatten them\n",
        "        chest_data[sensor_name] = data_array.flatten()\n",
        "\n",
        "df_s2_chest_data = pd.DataFrame(chest_data)\n",
        "\n",
        "# Process 'wrist' signals\n",
        "wrist_signals = s2_raw_data_dict['signal']['wrist']\n",
        "wrist_data = {}\n",
        "for sensor_name, data_array in wrist_signals.items():\n",
        "    if sensor_name == 'ACC':\n",
        "        # Accelerometer data is typically 3-axis, so create three columns\n",
        "        wrist_data['ACC_x'] = data_array[:, 0]\n",
        "        wrist_data['ACC_y'] = data_array[:, 1]\n",
        "        wrist_data['ACC_z'] = data_array[:, 2]\n",
        "    else:\n",
        "        # Other sensors might be 2D arrays with a single column, flatten them\n",
        "        wrist_data[sensor_name] = data_array.flatten()\n",
        "\n",
        "df_s2_wrist_data = pd.DataFrame(wrist_data)\n",
        "\n",
        "# Load the S2_quest.csv file into a pandas DataFrame\n",
        "df_s2_quest = pd.read_csv(f'{s2_data_path}/S2_quest.csv')\n",
        "\n",
        "print(\"df_s2_chest_data head:\")\n",
        "print(df_s2_chest_data.head())\n",
        "\n",
        "print(\"\\ndf_s2_wrist_data head:\")\n",
        "print(df_s2_wrist_data.head())\n",
        "\n",
        "print(\"\\ndf_s2_quest head:\")\n",
        "print(df_s2_quest.head())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b3c9fde6"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous code failed because sensor data arrays within 'wrist' (and likely 'chest') have different lengths due to varying sampling rates, making it impossible to directly create a single pandas DataFrame. To resolve the `ValueError: All arrays must be of the same length`, I will create separate DataFrames for each individual sensor signal (e.g., 'ACC', 'BVP', 'EDA', 'TEMP', 'ECG', 'EMG') and store them in dictionaries (`chest_dataframes` and `wrist_dataframes`) for organized access.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c05627fe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "outputId": "5f1ecc07-71f0-4be8-a146-1e1f53d25212"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Define the base path to the S2 subject data directory\n",
        "s2_data_path = '/content/WESAD_data/WESAD/S2'\n",
        "\n",
        "# Load the S2.pkl file into a dictionary first\n",
        "s2_raw_data_dict = pd.read_pickle(f'{s2_data_path}/S2.pkl')\n",
        "\n",
        "# Initialize dictionaries to hold individual sensor DataFrames\n",
        "chest_dataframes = {}\n",
        "wrist_dataframes = {}\n",
        "\n",
        "# Process 'chest' signals\n",
        "chest_signals = s2_raw_data_dict['signal']['chest']\n",
        "for sensor_name, data_array in chest_signals.items():\n",
        "    if sensor_name == 'ACC':\n",
        "        # Accelerometer data is 3-axis, create three columns\n",
        "        chest_dataframes[sensor_name] = pd.DataFrame(data_array, columns=['ACC_x', 'ACC_y', 'ACC_z'])\n",
        "    else:\n",
        "        # Other sensors are typically 1D or 2D with one column, flatten and create a DataFrame\n",
        "        chest_dataframes[sensor_name] = pd.DataFrame(data_array.flattenbase_data_path = '/content/WESAD_data/' # Corrected path\n",
        "         (), columns=[sensor_name])\n",
        "\n",
        "# Process 'wrist' signals\n",
        "wrist_signals = s2_raw_data_dict['signal']['wrist']\n",
        "for sensor_name, data_array in wrist_signals.items():\n",
        "    if sensor_name == 'ACC':\n",
        "        # Accelerometer data is 3-axis, create three columns\n",
        "        wrist_dataframes[sensor_name] = pd.DataFrame(data_array, columns=['ACC_x', 'ACC_y', 'ACC_z'])\n",
        "    else:\n",
        "        # Other sensors are typically 1D or 2D with one column, flatten and create a DataFrame\n",
        "        wrist_dataframes[sensor_name] = pd.DataFrame(data_array.flatten(), columns=[sensor_name])\n",
        "\n",
        "# Load the S2_quest.csv file into a pandas DataFrame\n",
        "df_s2_quest = pd.read_csv(f'{s2_data_path}/S2_quest.csv')\n",
        "\n",
        "print(\"Chest DataFrames (first 5 rows of each):\")\n",
        "for sensor_name, df in chest_dataframes.items():\n",
        "    print(f\"\\n{sensor_name}:\")\n",
        "    print(df.head())\n",
        "\n",
        "print(\"\\nWrist DataFrames (first 5 rows of each):\")\n",
        "for sensor_name, df in wrist_dataframes.items():\n",
        "    print(f\"\\n{sensor_name}:\")\n",
        "    print(df.head())\n",
        "\n",
        "print(\"\\ndf_s2_quest head:\")\n",
        "print(df_s2_quest.head())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/WESAD_data/WESAD/S2/S2.pkl'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-873747498.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Load the S2.pkl file into a dictionary first\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0ms2_raw_data_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_pickle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'{s2_data_path}/S2.pkl'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# Initialize dictionaries to hold individual sensor DataFrames\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/pickle.py\u001b[0m in \u001b[0;36mread_pickle\u001b[0;34m(filepath_or_buffer, compression, storage_options)\u001b[0m\n\u001b[1;32m    183\u001b[0m     \"\"\"\n\u001b[1;32m    184\u001b[0m     \u001b[0mexcs_to_catch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mAttributeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mModuleNotFoundError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m     with get_handle(\n\u001b[0m\u001b[1;32m    186\u001b[0m         \u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m         \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    880\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    881\u001b[0m             \u001b[0;31m# Binary mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 882\u001b[0;31m             \u001b[0mhandle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    883\u001b[0m         \u001b[0mhandles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    884\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/WESAD_data/WESAD/S2/S2.pkl'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "    import pandas as pd\n",
        "    import numpy as np\n",
        "    import os\n",
        "\n",
        "    # Initialize dictionaries to hold individual sensor DataFrames from ALL subjects\n",
        "    all_chest_dataframes = {}\n",
        "    all_wrist_dataframes = {}\n",
        "    all_quest_dataframes = {}\n",
        "\n",
        "    # Base path to the WESAD data directory\n",
        "    base_data_path = '/content/WESAD_data/WESAD'\n",
        "    print(f\"Base data path: {base_data_path}\")\n",
        "    print(f\"Does base path exist? {os.path.exists(base_data_path)}\")\n",
        "\n",
        "    # Loop through subjects S2 to S17\n",
        "    print(\"Starting loop through subjects S2-S17...\")\n",
        "    for subject_id in range(2, 18):\n",
        "        subject_name = f'S{subject_id}'\n",
        "        print(f\"Looking for subject: {subject_name}\")\n",
        "        subject_data_path = os.path.join(base_data_path, subject_name)\n",
        "        pkl_file_path = os.path.join(subject_data_path, f'{subject_name}.pkl')\n",
        "        quest_file_path = os.path.join(subject_data_path, f'{subject_name}_quest.csv')\n",
        "\n",
        "        print(f\"Checking for pkl file: {pkl_file_path}\")\n",
        "        if os.path.exists(pkl_file_path):\n",
        "            print(f\"Processing {subject_name} data...\")\n",
        "            # Load the .pkl file\n",
        "            raw_data_dict = pd.read_pickle(pkl_file_path)\n",
        "\n",
        "            # Process 'chest' signals\n",
        "            if 'chest' in raw_data_dict['signal']:\n",
        "                chest_signals = raw_data_dict['signal']['chest']\n",
        "                for sensor_name, data_array in chest_signals.items():\n",
        "                    key = f'{subject_name}_{sensor_name}'\n",
        "                    if sensor_name == 'ACC':\n",
        "                        all_chest_dataframes[key] = pd.DataFrame(data_array, columns=['ACC_x', 'ACC_y', 'ACC_z'])\n",
        "                    else:\n",
        "                        all_chest_dataframes[key] = pd.DataFrame(data_array.flatten(), columns=[sensor_name])\n",
        "\n",
        "            # Process 'wrist' signals\n",
        "            if 'wrist' in raw_data_dict['signal']:\n",
        "                wrist_signals = raw_data_dict['signal']['wrist']\n",
        "                for sensor_name, data_array in wrist_signals.items():\n",
        "                    key = f'{subject_name}_{sensor_name}'\n",
        "                    if sensor_name == 'ACC':\n",
        "                        all_wrist_dataframes[key] = pd.DataFrame(data_array, columns=['ACC_x', 'ACC_y', 'ACC_z'])\n",
        "                    else:\n",
        "                        all_wrist_dataframes[key] = pd.DataFrame(data_array.flatten(), columns=[sensor_name])\n",
        "\n",
        "            # Load the _quest.csv file\n",
        "            if os.path.exists(quest_file_path):\n",
        "                all_quest_dataframes[subject_name] = pd.read_csv(quest_file_path)\n",
        "                print(f\"Loaded {subject_name}_quest.csv\")\n",
        "            else:\n",
        "                print(f\"Warning: {subject_name}_quest.csv not found.\")\n",
        "\n",
        "        else:\n",
        "            print(f\"Warning: {subject_name}.pkl not found at {pkl_file_path}\")\n",
        "\n",
        "    print(\"\\nFinished loading data for all subjects.\")\n",
        "    print(\"Total chest dataframes loaded:\", len(all_chest_dataframes))\n",
        "    print(\"Total wrist dataframes loaded:\", len(all_wrist_dataframes))\n",
        "    print(\"Total quest dataframes loaded:\", len(all_quest_dataframes))\n",
        "\n",
        "    # For compatibility with the next cell, we'll make chest_dataframes and wrist_dataframes\n",
        "    # point to the new dictionaries. And df_s2_quest will be S2's quest df.\n",
        "    chest_dataframes = all_chest_dataframes\n",
        "    wrist_dataframes = all_wrist_dataframes\n",
        "    if 'S2' in all_quest_dataframes:\n",
        "        df_s2_quest = all_quest_dataframes['S2']\n",
        "    else:\n",
        "        df_s2_quest = None # Or handle error if S2 quest is essential\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M0DMTPNUCqW-",
        "outputId": "fe1c4799-02ab-4512-d376-d6329bfccd45"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Base data path: /content/WESAD_data/WESAD\n",
            "Does base path exist? False\n",
            "Starting loop through subjects S2-S17...\n",
            "Looking for subject: S2\n",
            "Checking for pkl file: /content/WESAD_data/WESAD/S2/S2.pkl\n",
            "Warning: S2.pkl not found at /content/WESAD_data/WESAD/S2/S2.pkl\n",
            "Looking for subject: S3\n",
            "Checking for pkl file: /content/WESAD_data/WESAD/S3/S3.pkl\n",
            "Warning: S3.pkl not found at /content/WESAD_data/WESAD/S3/S3.pkl\n",
            "Looking for subject: S4\n",
            "Checking for pkl file: /content/WESAD_data/WESAD/S4/S4.pkl\n",
            "Warning: S4.pkl not found at /content/WESAD_data/WESAD/S4/S4.pkl\n",
            "Looking for subject: S5\n",
            "Checking for pkl file: /content/WESAD_data/WESAD/S5/S5.pkl\n",
            "Warning: S5.pkl not found at /content/WESAD_data/WESAD/S5/S5.pkl\n",
            "Looking for subject: S6\n",
            "Checking for pkl file: /content/WESAD_data/WESAD/S6/S6.pkl\n",
            "Warning: S6.pkl not found at /content/WESAD_data/WESAD/S6/S6.pkl\n",
            "Looking for subject: S7\n",
            "Checking for pkl file: /content/WESAD_data/WESAD/S7/S7.pkl\n",
            "Warning: S7.pkl not found at /content/WESAD_data/WESAD/S7/S7.pkl\n",
            "Looking for subject: S8\n",
            "Checking for pkl file: /content/WESAD_data/WESAD/S8/S8.pkl\n",
            "Warning: S8.pkl not found at /content/WESAD_data/WESAD/S8/S8.pkl\n",
            "Looking for subject: S9\n",
            "Checking for pkl file: /content/WESAD_data/WESAD/S9/S9.pkl\n",
            "Warning: S9.pkl not found at /content/WESAD_data/WESAD/S9/S9.pkl\n",
            "Looking for subject: S10\n",
            "Checking for pkl file: /content/WESAD_data/WESAD/S10/S10.pkl\n",
            "Warning: S10.pkl not found at /content/WESAD_data/WESAD/S10/S10.pkl\n",
            "Looking for subject: S11\n",
            "Checking for pkl file: /content/WESAD_data/WESAD/S11/S11.pkl\n",
            "Warning: S11.pkl not found at /content/WESAD_data/WESAD/S11/S11.pkl\n",
            "Looking for subject: S12\n",
            "Checking for pkl file: /content/WESAD_data/WESAD/S12/S12.pkl\n",
            "Warning: S12.pkl not found at /content/WESAD_data/WESAD/S12/S12.pkl\n",
            "Looking for subject: S13\n",
            "Checking for pkl file: /content/WESAD_data/WESAD/S13/S13.pkl\n",
            "Warning: S13.pkl not found at /content/WESAD_data/WESAD/S13/S13.pkl\n",
            "Looking for subject: S14\n",
            "Checking for pkl file: /content/WESAD_data/WESAD/S14/S14.pkl\n",
            "Warning: S14.pkl not found at /content/WESAD_data/WESAD/S14/S14.pkl\n",
            "Looking for subject: S15\n",
            "Checking for pkl file: /content/WESAD_data/WESAD/S15/S15.pkl\n",
            "Warning: S15.pkl not found at /content/WESAD_data/WESAD/S15/S15.pkl\n",
            "Looking for subject: S16\n",
            "Checking for pkl file: /content/WESAD_data/WESAD/S16/S16.pkl\n",
            "Warning: S16.pkl not found at /content/WESAD_data/WESAD/S16/S16.pkl\n",
            "Looking for subject: S17\n",
            "Checking for pkl file: /content/WESAD_data/WESAD/S17/S17.pkl\n",
            "Warning: S17.pkl not found at /content/WESAD_data/WESAD/S17/S17.pkl\n",
            "\n",
            "Finished loading data for all subjects.\n",
            "Total chest dataframes loaded: 0\n",
            "Total wrist dataframes loaded: 0\n",
            "Total quest dataframes loaded: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "    import pandas as pd\n",
        "    import numpy as np\n",
        "    from scipy.signal import resample\n",
        "\n",
        "    # Target frequency for resampling\n",
        "    TARGET_FREQ = 32  # Hz\n",
        "    TOLERANCE = '10ms' # Tolerance for nearest matching\n",
        "\n",
        "    # --- Resample Chest Data (Downsampling) ---\n",
        "    resampled_chest_dataframes = {}\n",
        "    for sensor_name, df in chest_dataframes.items():\n",
        "        if not df.empty and 'ACC' in sensor_name: # ACC is 700Hz\n",
        "            original_len = len(df)\n",
        "            if original_len > 1:\n",
        "                time_index = pd.to_timedelta(np.arange(original_len) / 700.0, unit='s')\n",
        "                df.index = time_index\n",
        "                resample_len = int(original_len * TARGET_FREQ / 700.0)\n",
        "                resampled_data = {}\n",
        "                for col in df.columns:\n",
        "                    resampled_data[col] = resample(df[col], resample_len)\n",
        "                resampled_df = pd.DataFrame(resampled_data)\n",
        "                resampled_time_index = pd.to_timedelta(np.arange(resample_len) / float(TARGET_FREQ), unit='s')\n",
        "                resampled_df.index = resampled_time_index\n",
        "                resampled_chest_dataframes[sensor_name] = resampled_df\n",
        "                print(f\"Resampled {sensor_name} from {original_len} to {resample_len} points (700Hz to {TARGET_FREQ}Hz)\")\n",
        "            else:\n",
        "                resampled_chest_dataframes[sensor_name] = df.copy()\n",
        "                print(f\"Kept {sensor_name} as is (empty or single row)\")\n",
        "        else:\n",
        "            resampled_chest_dataframes[sensor_name] = df.copy()\n",
        "            if not df.empty:\n",
        "                 original_len = len(df)\n",
        "                 time_index = pd.to_timedelta(np.arange(original_len) / float(TARGET_FREQ), unit='s')\n",
        "                 df.index = time_index\n",
        "                 resampled_chest_dataframes[sensor_name].index = time_index\n",
        "                 print(f\"Kept non-ACC {sensor_name} with {original_len} points, assumed {TARGET_FREQ}Hz\")\n",
        "            else:\n",
        "                print(f\"Kept non-ACC {sensor_name} as is (empty)\")\n",
        "\n",
        "    # --- Resample Wrist Data (Upsampling/Downsampling) ---\n",
        "    resampled_wrist_dataframes = {}\n",
        "    wrist_original_freqs = {'ACC': 32, 'BVP': 64, 'EDA': 4, 'TEMP': 4}\n",
        "\n",
        "    for sensor_name, df in wrist_dataframes.items():\n",
        "        if not df.empty and sensor_name in wrist_original_freqs:\n",
        "            original_freq = wrist_original_freqs[sensor_name]\n",
        "            original_len = len(df)\n",
        "            if original_len > 1 and original_freq != TARGET_FREQ:\n",
        "                time_index = pd.to_timedelta(np.arange(original_len) / float(original_freq), unit='s')\n",
        "                df.index = time_index\n",
        "                resample_len = int(original_len * TARGET_FREQ / float(original_freq))\n",
        "                resampled_data = {}\n",
        "                for col in df.columns:\n",
        "                    resampled_data[col] = resample(df[col], resample_len)\n",
        "                resampled_df = pd.DataFrame(resampled_data)\n",
        "                resampled_time_index = pd.to_timedelta(np.arange(resample_len) / float(TARGET_FREQ), unit='s')\n",
        "                resampled_df.index = resampled_time_index\n",
        "                resampled_wrist_dataframes[sensor_name] = resampled_df\n",
        "                print(f\"Resampled {sensor_name} from {original_len} to {resample_len} points ({original_freq}Hz to {TARGET_FREQ}Hz)\")\n",
        "            else:\n",
        "                if original_len > 0:\n",
        "                    time_index = pd.to_timedelta(np.arange(original_len) / float(original_freq), unit='s')\n",
        "                    df.index = time_index\n",
        "                    resampled_wrist_dataframes[sensor_name] = df.copy()\n",
        "                    print(f\"Kept {sensor_name} as is with {original_len} points (at/near {TARGET_FREQ}Hz or single row)\")\n",
        "                else:\n",
        "                    resampled_wrist_dataframes[sensor_name] = df.copy()\n",
        "                    print(f\"Kept {sensor_name} as is (empty)\")\n",
        "        else:\n",
        "            resampled_wrist_dataframes[sensor_name] = df.copy()\n",
        "            print(f\"Kept {sensor_name} as is (empty or not in freq map)\")\n",
        "\n",
        "    print(\"\\nResampling complete.\")\n",
        "\n",
        "    # --- Alignment ---\n",
        "    # Assuming 'event_timings' df exists and has 'Start_Time' and 'End_Time' as datetime objects\n",
        "    # If event_timings is not loaded/defined, this part will fail and needs event_timings loaded first.\n",
        "    # For now, let's assume it's loaded from df_s2_quest or similar and converted.\n",
        "    # We need to get event_timings first. Let's look for df_s2_quest processing\n",
        "    # For demonstration, let's create dummy event_timings if not found\n",
        "    if 'df_event_timings' not in locals() and 'df_event_timings' not in globals():\n",
        "        print(\"Warning: 'df_event_timings' not found. Using dummy timings for alignment demonstration.\")\n",
        "        # Extract from df_s2_quest if it exists, otherwise dummy\n",
        "        if 'df_s2_quest' in locals() or 'df_s2_quest' in globals():\n",
        "             start_time_str = df_s2_quest.iloc[1,0].split(': ')[1] # Assuming # START: time is at index 1\n",
        "             end_time_str = df_s2_quest.iloc[2,0].split(': ')[1]   # Assuming # END: time is at index 2\n",
        "             start_time = pd.to_datetime(start_time_str)\n",
        "             end_time = pd.to_datetime(end_time_str)\n",
        "             df_event_timings = pd.DataFrame({'Start_Time': [start_time], 'End_Time': [end_time]})\n",
        "        else:\n",
        "            start_time = pd.to_datetime('1970-01-01 00:00:00') # Dummy start\n",
        "            end_time = pd.to_datetime('1970-01-01 00:05:00')   # Dummy end (5 mins later)\n",
        "            df_event_timings = pd.DataFrame({'Start_Time': [start_time], 'End_Time': [end_time]})\n",
        "\n",
        "\n",
        "    start_time = df_event_timings['Start_Time'].iloc[0]\n",
        "    end_time = df_event_timings['End_Time'].iloc[0]\n",
        "\n",
        "    aligned_data = {}\n",
        "\n",
        "    # Align resampled chest data\n",
        "    for sensor_name, df in resampled_chest_dataframes.items():\n",
        "        if not df.empty and isinstance(df.index, pd.TimedeltaIndex):\n",
        "            df_abs_time = df.copy()\n",
        "            df_abs_time.index = start_time + df_abs_time.index\n",
        "            aligned_df = df_abs_time[(df_abs_time.index >= start_time) & (df_abs_time.index <= end_time)]\n",
        "            # Make index relative to start_time again for merging\n",
        "            aligned_df.index = aligned_df.index - start_time\n",
        "            aligned_data[f'chest_{sensor_name}'] = aligned_df\n",
        "            print(f\"Aligned chest_{sensor_name}\")\n",
        "\n",
        "    # Align resampled wrist data\n",
        "    for sensor_name, df in resampled_wrist_dataframes.items():\n",
        "        if not df.empty and isinstance(df.index, pd.TimedeltaIndex):\n",
        "            df_abs_time = df.copy()\n",
        "            df_abs_time.index = start_time + df_abs_time.index\n",
        "            aligned_df = df_abs_time[(df_abs_time.index >= start_time) & (df_abs_time.index <= end_time)]\n",
        "            aligned_df.index = aligned_df.index - start_time\n",
        "            aligned_data[f'wrist_{sensor_name}'] = aligned_df\n",
        "            print(f\"Aligned wrist_{sensor_name}\")\n",
        "\n",
        "    # --- Consolidation ---\n",
        "    df_final = pd.DataFrame()\n",
        "    for sensor_key, df in aligned_data.items():\n",
        "        if not df.empty:\n",
        "            df_renamed = df.rename(columns={col: f'{sensor_key}_{col}' for col in df.columns})\n",
        "            if df_final.empty:\n",
        "                df_final = df_renamed\n",
        "            else:\n",
        "                df_final = pd.merge_asof(df_final.sort_index(), df_renamed.sort_index(), left_index=True, right_index=True, direction='nearest', tolerance=pd.Timedelta(TOLERANCE))\n",
        "\n",
        "    print(\"\\nAlignment and Consolidation complete.\")\n",
        "    print(\"Final DataFrame head:\")\n",
        "    print(df_final.head())\n",
        "    print(\"Final DataFrame info:\")\n",
        "    df_final.info()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "pDmEu1G6BKNg",
        "outputId": "3bfa58da-bd5c-4125-8566-3718ca62fbaa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'chest_dataframes' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3851193473.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# --- Resample Chest Data (Downsampling) ---\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mresampled_chest_dataframes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0msensor_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mchest_dataframes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m'ACC'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msensor_name\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# ACC is 700Hz\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0moriginal_len\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'chest_dataframes' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b0b99604"
      },
      "source": [
        "## Initial Data Inspection\n",
        "\n",
        "### Subtask:\n",
        "Perform an initial inspection of the loaded data, including checking the first few rows (.head()), data types (.info()), and descriptive statistics (.describe()) to identify potential issues.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7daef674"
      },
      "source": [
        "**Reasoning**:\n",
        "To perform an initial inspection, I will iterate through the `chest_dataframes` and `wrist_dataframes` dictionaries, applying `.head()`, `.info()`, and `.describe()` to each sensor DataFrame. I will also apply these methods to the `df_s2_quest` DataFrame.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3687c834"
      },
      "source": [
        "print(\"--- Initial Data Inspection for Chest Sensor DataFrames ---\")\n",
        "for sensor_name, df in chest_dataframes.items():\n",
        "    print(f\"\\n----- {sensor_name} Data (Chest) -----\")\n",
        "    print(f\"\\n{sensor_name}.head():\")\n",
        "    print(df.head())\n",
        "    print(f\"\\n{sensor_name}.info():\")\n",
        "    df.info()\n",
        "    print(f\"\\n{sensor_name}.describe():\")\n",
        "    print(df.describe())\n",
        "\n",
        "print(\"\\n--- Initial Data Inspection for Wrist Sensor DataFrames ---\")\n",
        "for sensor_name, df in wrist_dataframes.items():\n",
        "    print(f\"\\n----- {sensor_name} Data (Wrist) -----\")\n",
        "    print(f\"\\n{sensor_name}.head():\")\n",
        "    print(df.head())\n",
        "    print(f\"\\n{sensor_name}.info():\")\n",
        "    df.info()\n",
        "    print(f\"\\n{sensor_name}.describe():\")\n",
        "    print(df.describe())\n",
        "\n",
        "print(\"\\n--- Initial Data Inspection for Questionnaire Data (df_s2_quest) ---\")\n",
        "print(\"\\ndf_s2_quest.head():\")\n",
        "print(df_s2_quest.head())\n",
        "print(\"\\ndf_s2_quest.info():\")\n",
        "df_s2_quest.info()\n",
        "print(\"\\ndf_s2_quest.describe():\")\n",
        "print(df_s2_quest.describe())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "65039e93"
      },
      "source": [
        "## Handle Missing Values\n",
        "\n",
        "### Subtask:\n",
        "Identify and address any missing values in the dataset. This may involve imputation, deletion of rows/columns, or other appropriate strategies based on the extent and nature of missing data.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9a7363d1"
      },
      "source": [
        "**Reasoning**:\n",
        "To identify missing values, I will iterate through each DataFrame in the `chest_dataframes` and `wrist_dataframes` dictionaries and print the sum of null values for each column. I will also add a markdown block to acknowledge that `df_s2_quest` needs parsing before checking for missing values.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "68f3c6ca"
      },
      "source": [
        "print(\"--- Checking for Missing Values in Chest Sensor DataFrames ---\")\n",
        "for sensor_name, df in chest_dataframes.items():\n",
        "    print(f\"\\nMissing values in {sensor_name} (Chest):\")\n",
        "    print(df.isnull().sum())\n",
        "\n",
        "print(\"\\n--- Checking for Missing Values in Wrist Sensor DataFrames ---\")\n",
        "for sensor_name, df in wrist_dataframes.items():\n",
        "    print(f\"\\nMissing values in {sensor_name} (Wrist):\")\n",
        "    print(df.isnull().sum())\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "38cd41c5"
      },
      "source": [
        "```markdown\n",
        "**Regarding `df_s2_quest`:**\n",
        "\n",
        "The `df_s2_quest` DataFrame is not yet in a tabular format suitable for a direct `.isnull().sum()` check. It contains metadata and questionnaire responses within a few rows, separated by semicolons. This DataFrame will require further parsing and cleaning before a meaningful assessment of missing values can be performed for individual questions/fields.\n",
        "\n",
        "Based on the current state of the chest and wrist sensor data, there are no missing values detected.\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ee336d69"
      },
      "source": [
        "## Handle Duplicates\n",
        "\n",
        "### Subtask:\n",
        "Check for and remove any duplicate rows in the dataset to ensure data integrity.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a3de8ef4"
      },
      "source": [
        "**Reasoning**:\n",
        "To identify duplicate rows, I will iterate through each DataFrame in the `chest_dataframes` and `wrist_dataframes` dictionaries and use the `.duplicated().sum()` method to count and print the number of duplicate rows for each sensor.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3df1e040"
      },
      "source": [
        "print(\"--- Checking for Duplicate Rows in Chest Sensor DataFrames ---\")\n",
        "for sensor_name, df in chest_dataframes.items():\n",
        "    print(f\"\\nDuplicate rows in {sensor_name} (Chest): {df.duplicated().sum()}\")\n",
        "\n",
        "print(\"\\n--- Checking for Duplicate Rows in Wrist Sensor DataFrames ---\")\n",
        "for sensor_name, df in wrist_dataframes.items():\n",
        "    print(f\"\\nDuplicate rows in {sensor_name} (Wrist): {df.duplicated().sum()}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0d5b5d32"
      },
      "source": [
        "```markdown\n",
        "**Regarding `df_s2_quest`:**\n",
        "\n",
        "The `df_s2_quest` DataFrame is not yet in a tabular format suitable for a direct `.duplicated().sum()` check. It contains metadata and questionnaire responses within a few rows, separated by semicolons. This DataFrame will require further parsing and cleaning before a meaningful assessment of duplicate rows can be performed for individual questions/fields.\n",
        "\n",
        "**Regarding Sensor DataFrames:**\n",
        "\n",
        "Many of the chest and wrist sensor DataFrames contain a significant number of duplicate rows. This will need to be addressed in the next step to ensure data integrity and prevent skewed analysis.\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "97ab4e54"
      },
      "source": [
        "**Regarding `df_s2_quest`:**\n",
        "\n",
        "The `df_s2_quest` DataFrame is not yet in a tabular format suitable for a direct `.duplicated().sum()` check. It contains metadata and questionnaire responses within a few rows, separated by semicolons. This DataFrame will require further parsing and cleaning before a meaningful assessment of duplicate rows can be performed for individual questions/fields.\n",
        "\n",
        "**Regarding Sensor DataFrames:**\n",
        "\n",
        "Many of the chest and wrist sensor DataFrames contain a significant number of duplicate rows. This will need to be addressed in the next step to ensure data integrity and prevent skewed analysis."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "47c12422"
      },
      "source": [
        "**Reasoning**:\n",
        "Many sensor dataframes were found to contain duplicate rows, which can affect data integrity. To address this, I will iterate through each DataFrame in the `chest_dataframes` and `wrist_dataframes` dictionaries and use the `.drop_duplicates()` method to remove all duplicate rows. After removal, I will confirm by re-checking the count of duplicate rows for each DataFrame.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1f37cf66"
      },
      "source": [
        "print(\"--- Removing Duplicate Rows from Chest Sensor DataFrames ---\")\n",
        "for sensor_name, df in chest_dataframes.items():\n",
        "    initial_duplicates = df.duplicated().sum()\n",
        "    if initial_duplicates > 0:\n",
        "        chest_dataframes[sensor_name] = df.drop_duplicates().reset_index(drop=True)\n",
        "        print(f\"Removed {initial_duplicates} duplicate rows from {sensor_name} (Chest). New count: {chest_dataframes[sensor_name].duplicated().sum()}\")\n",
        "    else:\n",
        "        print(f\"No duplicate rows found in {sensor_name} (Chest).\")\n",
        "\n",
        "print(\"\\n--- Removing Duplicate Rows from Wrist Sensor DataFrames ---\")\n",
        "for sensor_name, df in wrist_dataframes.items():\n",
        "    initial_duplicates = df.duplicated().sum()\n",
        "    if initial_duplicates > 0:\n",
        "        wrist_dataframes[sensor_name] = df.drop_duplicates().reset_index(drop=True)\n",
        "        print(f\"Removed {initial_duplicates} duplicate rows from {sensor_name} (Wrist). New count: {wrist_dataframes[sensor_name].duplicated().sum()}\")\n",
        "    else:\n",
        "        print(f\"No duplicate rows found in {sensor_name} (Wrist).\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e78b71f6"
      },
      "source": [
        "## Check Data Types\n",
        "\n",
        "### Subtask:\n",
        "Verify that all columns have appropriate data types (e.g., numeric, categorical, datetime) and convert them if necessary to ensure correct processing in subsequent analysis.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f372265b"
      },
      "source": [
        "**Reasoning**:\n",
        "To verify data types, I will iterate through each DataFrame in `chest_dataframes` and `wrist_dataframes` and print their data types and information using `.info()`. I will also print the `.info()` for `df_s2_quest` and acknowledge that it requires parsing.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cf215c17"
      },
      "source": [
        "print(\"--- Checking Data Types for Chest Sensor DataFrames ---\")\n",
        "for sensor_name, df in chest_dataframes.items():\n",
        "    print(f\"\\n----- {sensor_name} Data (Chest) -----\")\n",
        "    df.info()\n",
        "\n",
        "print(\"\\n--- Checking Data Types for Wrist Sensor DataFrames ---\")\n",
        "for sensor_name, df in wrist_dataframes.items():\n",
        "    print(f\"\\n----- {sensor_name} Data (Wrist) -----\")\n",
        "    df.info()\n",
        "\n",
        "print(\"\\n--- Checking Data Types for Questionnaire Data (df_s2_quest) ---\")\n",
        "df_s2_quest.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d4b97275"
      },
      "source": [
        "## Final Task\n",
        "\n",
        "### Subtask:\n",
        "Summarize the data cleaning steps performed and prepare the cleaned data for further analysis or modeling.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3850aa77"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "\n",
        "*   **Data Structure Identification**: The WESAD dataset is organized into subject-specific subdirectories (e.g., `S2/`), each containing a `.pkl` file for processed sensor data, a `_quest.csv` file for questionnaire data, and a `_E4_Data.zip` file for raw data.\n",
        "*   **Data Loading Strategy**: Due to varying sampling rates and data structures within the `S2.pkl` file, individual pandas DataFrames were created for each chest and wrist sensor (e.g., `ACC`, `ECG`, `BVP`, `EDA`, `Temp`). Accelerometer data was specifically handled to separate its x, y, and z components into distinct columns. The `S2_quest.csv` file was loaded into a separate DataFrame.\n",
        "*   **Sensor Data Consistency**:\n",
        "    *   **Chest Sensors**: All chest sensor DataFrames (`ACC`, `ECG`, `EMG`, `EDA`, `Temp`, `Resp`) have a consistent length of 4,255,300 entries, indicating uniform sampling and duration. Their data types are primarily `float64`, with `Temp` being `float32`.\n",
        "    *   **Wrist Sensors**: Wrist sensor DataFrames show varying lengths, suggesting different sampling rates or recording durations (e.g., `ACC` with 194,528 entries, `BVP` with 389,056 entries, `EDA` and `TEMP` with 24,316 entries). All wrist sensor data are `float64`.\n",
        "*   **Missing Values**: No missing values were found in any of the chest or wrist sensor DataFrames.\n",
        "*   **Duplicate Rows**: A significant number of duplicate rows were initially identified across all sensor DataFrames:\n",
        "    *   Chest sensors: Ranged from 470,641 (ACC) to 4,251,080 (Temp).\n",
        "    *   Wrist sensors: Ranged from 22,615 (EDA) to 343,616 (BVP).\n",
        "    *   All identified duplicate rows were successfully removed, resulting in zero duplicates in the cleaned sensor DataFrames.\n",
        "*   **Questionnaire Data (`df_s2_quest`) State**: The `df_s2_quest` DataFrame was loaded as a single `object` type column. It is not yet in a tabular format, requiring further parsing to extract meaningful questionnaire data and to perform checks for missing values or duplicates within its content.\n",
        "\n",
        "### Insights or Next Steps\n",
        "\n",
        "*   **Parse Questionnaire Data**: The `df_s2_quest` DataFrame needs to be parsed and transformed into a structured tabular format to make its contents accessible for analysis and to properly assess its data quality (e.g., missing values, data types).\n",
        "*   **Synchronize Sensor Data**: Given the different sampling rates and lengths of the wrist sensor DataFrames (and potentially between chest and wrist), future steps should focus on synchronizing these time-series datasets, possibly through resampling or alignment, to enable integrated analysis.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cab14871"
      },
      "source": [
        "## Save Cleaned Data\n",
        "\n",
        "### Subtask:\n",
        "Save the cleaned `df_event_timings`, `df_questionnaire_responses`, and the individual sensor DataFrames (from `chest_dataframes` and `wrist_dataframes`) to CSV files for future use. This will involve creating a new directory for cleaned data and then exporting each DataFrame."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9742999c"
      },
      "source": [
        "**Reasoning**:\n",
        "To save the cleaned data, I will first define a path for a new 'cleaned_data' directory within the `S2` subject folder. I will then create this directory if it doesn't already exist. Afterward, I will iterate through the `chest_dataframes` and `wrist_dataframes` dictionaries, saving each sensor DataFrame as a CSV file. Finally, I will save `df_event_timings` and `df_questionnaire_responses` as separate CSV files to the same directory, ensuring all processed data is persistently stored."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "875bf236"
      },
      "source": [
        "import os\n",
        "\n",
        "# Define the directory to save cleaned data\n",
        "cleaned_data_dir = f'{s2_data_path}/cleaned_data'\n",
        "\n",
        "# Create the directory if it does not exist\n",
        "os.makedirs(cleaned_data_dir, exist_ok=True)\n",
        "\n",
        "print(f\"Saving cleaned data to: {cleaned_data_dir}\")\n",
        "\n",
        "# Save df_event_timings\n",
        "df_event_timings.to_csv(f'{cleaned_data_dir}/df_event_timings.csv', index=False)\n",
        "print(\"Saved df_event_timings.csv\")\n",
        "\n",
        "# Save df_questionnaire_responses\n",
        "df_questionnaire_responses.to_csv(f'{cleaned_data_dir}/df_questionnaire_responses.csv', index=False)\n",
        "print(\"Saved df_questionnaire_responses.csv\")\n",
        "\n",
        "# Save individual chest sensor DataFrames\n",
        "for sensor_name, df in chest_dataframes.items():\n",
        "    df.to_csv(f'{cleaned_data_dir}/chest_{sensor_name.lower()}.csv', index=False)\n",
        "    print(f\"Saved chest_{sensor_name.lower()}.csv\")\n",
        "\n",
        "# Save individual wrist sensor DataFrames\n",
        "for sensor_name, df in wrist_dataframes.items():\n",
        "    df.to_csv(f'{cleaned_data_dir}/wrist_{sensor_name.lower()}.csv', index=False)\n",
        "    print(f\"Saved wrist_{sensor_name.lower()}.csv\")\n",
        "\n",
        "print(\"All cleaned DataFrames saved successfully!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9e0dbd82"
      },
      "source": [
        "# Task\n",
        "Inspect the `df_s2_quest` DataFrame to understand its structure for parsing, including printing additional rows and specific slices to identify patterns in the header information and data rows."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8d3745c3"
      },
      "source": [
        "## Inspect df_s2_quest structure\n",
        "\n",
        "### Subtask:\n",
        "Further inspect the `df_s2_quest` to understand its exact structure and identify patterns for parsing, especially header information and data rows. This may involve printing more rows or specific slices of the DataFrame.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7e9951ae"
      },
      "source": [
        "**Reasoning**:\n",
        "To understand the structure of `df_s2_quest` for parsing, I will display all its rows, then specifically print the first row and a few other distinct rows to identify patterns in how different types of information are presented.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ace49612"
      },
      "source": [
        "print(\"--- Displaying all rows of df_s2_quest ---\")\n",
        "print(df_s2_quest.to_string())\n",
        "\n",
        "print(\"\\n--- First row of df_s2_quest ---\")\n",
        "print(df_s2_quest.iloc[0])\n",
        "\n",
        "print(\"\\n--- Second row of df_s2_quest ---\")\n",
        "print(df_s2_quest.iloc[1])\n",
        "\n",
        "print(\"\\n--- Third row of df_s2_quest ---\")\n",
        "print(df_s2_quest.iloc[2])\n",
        "\n",
        "print(\"\\n--- Fifth row of df_s2_quest (index 4) ---\")\n",
        "print(df_s2_quest.iloc[4])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0d715bcd"
      },
      "source": [
        "The inspection of `df_s2_quest` has provided a clear understanding of its structure:\n",
        "\n",
        "*   **Single Column Format**: The entire CSV content is loaded into a single column, with each row being a string containing semicolon-separated values.\n",
        "*   **Header Rows**: The first few rows (`#ORDER`, `#START`, `#END`) provide metadata about the experimental setup (e.g., order of tasks, start and end times). These rows contain both text labels and numeric values.\n",
        "*   **Questionnaire Data Rows**: Subsequent rows (`#PANAS`, `#STAI`, `#DIM`, `#SSSQ`) contain responses to different questionnaires. Each of these lines starts with a tag (e.g., `#PANAS`) followed by a series of numeric responses, also semicolon-separated.\n",
        "*   **Empty Rows**: There are rows containing only semicolons (`;;;;...`) which act as separators between different sections of the questionnaire data.\n",
        "*   **Delimiter**: The primary delimiter for all values appears to be a semicolon (`;`).\n",
        "*   **Subject Identifier**: The very first line `\"# Subj;S2;;;;;;;;;;;;;;;;;;;;;;;;;\"` identifies the subject. This information was implicitly captured when loading the `S2_quest.csv` file.\n",
        "\n",
        "**Next Steps for Parsing `df_s2_quest`:**\n",
        "\n",
        "1.  **Extract Metadata**: Parse the `#ORDER`, `#START`, and `#END` rows to extract the task sequence and their corresponding start/end times. This data could be stored in a dictionary or a separate DataFrame.\n",
        "2.  **Extract Questionnaire Data**: Iterate through the remaining rows, identify each questionnaire type (PANAS, STAI, DIM, SSSQ), and extract the numerical responses into separate pandas DataFrames, possibly with appropriate column names derived from the questionnaire structure (if available in documentation or inferable).\n",
        "3.  **Clean and Convert Data Types**: Ensure extracted numerical data is converted to appropriate numeric types (e.g., `int` or `float`).\n",
        "4.  **Handle Empty/Irrelevant Values**: Remove or appropriately handle the trailing empty fields (`;;`) and the empty separator rows.\n",
        "\n",
        "This detailed understanding of the `df_s2_quest` structure will guide the next data cleaning steps to transform this raw questionnaire data into a usable tabular format."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "29f1e7c2"
      },
      "source": [
        "## Extract event timings\n",
        "\n",
        "### Subtask:\n",
        "Parse the 'START' and 'END' rows from `df_s2_quest` to extract the event timings for different experimental phases (e.g., Base, TSST, Medi 1, etc.) and store them in a suitable data structure, like a dictionary or a small DataFrame.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "59e8f17d"
      },
      "source": [
        "**Reasoning**:\n",
        "To extract event timings, I will locate the '# ORDER', '# START', and '# END' rows, split their string content by semicolons, clean the resulting lists by removing empty strings and the initial '#' character, create event names from the '# ORDER' row, and then construct dictionaries for 'START' and 'END' times. Finally, I will combine these into a new pandas DataFrame as instructed.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4f76da6b"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "# 1. Locate and extract the string content from the relevant rows\n",
        "order_str = df_s2_quest.iloc[0, 0]\n",
        "start_str = df_s2_quest.iloc[1, 0]\n",
        "end_str = df_s2_quest.iloc[2, 0]\n",
        "\n",
        "# 2. Split by semicolon and clean the lists\n",
        "# Remove empty strings and the initial '#' from the first element\n",
        "\n",
        "def clean_split_list(s):\n",
        "    # Remove the first '#' if present and split\n",
        "    parts = s.replace('#', '').split(';')\n",
        "    # Filter out empty strings\n",
        "    return [p.strip() for p in parts if p.strip()]\n",
        "\n",
        "cleaned_order = clean_split_list(order_str)\n",
        "cleaned_start = clean_split_list(start_str)\n",
        "cleaned_end = clean_split_list(end_str)\n",
        "\n",
        "# 3. Create a list of event names (skipping the 'ORDER' label)\n",
        "event_names = cleaned_order[1:]\n",
        "\n",
        "# 4. Create dictionaries for START and END times, converting to float\n",
        "start_times = {}\n",
        "for i, event in enumerate(event_names):\n",
        "    # Ensure index is within bounds for start_str data, skipping the 'START' label\n",
        "    if (i + 1) < len(cleaned_start):\n",
        "        try:\n",
        "            start_times[event] = float(cleaned_start[i + 1])\n",
        "        except ValueError:\n",
        "            start_times[event] = None # Handle cases where conversion to float fails\n",
        "\n",
        "end_times = {}\n",
        "for i, event in enumerate(event_names):\n",
        "    # Ensure index is within bounds for end_str data, skipping the 'END' label\n",
        "    if (i + 1) < len(cleaned_end):\n",
        "        try:\n",
        "            end_times[event] = float(cleaned_end[i + 1])\n",
        "        except ValueError:\n",
        "            end_times[event] = None # Handle cases where conversion to float fails\n",
        "\n",
        "# 5. Combine into a new pandas DataFrame\n",
        "# Create lists for DataFrame construction\n",
        "events_list = []\n",
        "start_time_list = []\n",
        "end_time_list = []\n",
        "\n",
        "for event in event_names:\n",
        "    events_list.append(event)\n",
        "    start_time_list.append(start_times.get(event))\n",
        "    end_time_list.append(end_times.get(event))\n",
        "\n",
        "df_event_timings = pd.DataFrame({\n",
        "    'Event': events_list,\n",
        "    'Start_Time': start_time_list,\n",
        "    'End_Time': end_time_list\n",
        "})\n",
        "\n",
        "print(\"Event Timings DataFrame:\")\n",
        "print(df_event_timings)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "99cf2a62"
      },
      "source": [
        "## Extract questionnaire responses\n",
        "\n",
        "### Subtask:\n",
        "Parse the 'PANAS', 'STAI', 'DIM', and 'SSSQ' questionnaire response rows from `df_s2_quest`.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "34449d2b"
      },
      "source": [
        "**Reasoning**:\n",
        "To parse the questionnaire responses, I will iterate through the `df_s2_quest` DataFrame, identify rows corresponding to 'PANAS', 'STAI', 'DIM', and 'SSSQ' questionnaires, extract the numeric data by splitting the strings and converting values to float, and then store these responses into separate pandas DataFrames as instructed.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b0ee8355"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Initialize empty lists to store parsed responses for each questionnaire type\n",
        "panas_data = []\n",
        "stai_data = []\n",
        "dim_data = []\n",
        "sssq_data = []\n",
        "\n",
        "# Iterate through df_s2_quest from row index 4 onwards (as metadata is already extracted)\n",
        "for i in range(4, len(df_s2_quest)):\n",
        "    row_string = df_s2_quest.iloc[i, 0]\n",
        "\n",
        "    # Check if the row starts with a questionnaire identifier and process it\n",
        "    if row_string.startswith('# PANAS'):\n",
        "        # Remove the tag and split by semicolon, then convert to numeric\n",
        "        parts = row_string.replace('# PANAS;', '').split(';')\n",
        "        numeric_parts = [float(p.strip()) for p in parts if p.strip()]\n",
        "        if numeric_parts:\n",
        "            panas_data.append(numeric_parts)\n",
        "    elif row_string.startswith('# STAI'):\n",
        "        parts = row_string.replace('# STAI;', '').split(';')\n",
        "        numeric_parts = [float(p.strip()) for p in parts if p.strip()]\n",
        "        if numeric_parts:\n",
        "            stai_data.append(numeric_parts)\n",
        "    elif row_string.startswith('# DIM'):\n",
        "        parts = row_string.replace('# DIM;', '').split(';')\n",
        "        numeric_parts = [float(p.strip()) for p in parts if p.strip()]\n",
        "        if numeric_parts:\n",
        "            dim_data.append(numeric_parts)\n",
        "    elif row_string.startswith('# SSSQ'):\n",
        "        parts = row_string.replace('# SSSQ;', '').split(';')\n",
        "        numeric_parts = [float(p.strip()) for p in parts if p.strip()]\n",
        "        if numeric_parts:\n",
        "            sssq_data.append(numeric_parts)\n",
        "\n",
        "# Convert lists of lists to pandas DataFrames. pd.DataFrame handles varying row lengths by padding with NaN.\n",
        "df_panas = pd.DataFrame(panas_data)\n",
        "df_stai = pd.DataFrame(stai_data)\n",
        "df_dim = pd.DataFrame(dim_data)\n",
        "df_sssq = pd.DataFrame(sssq_data)\n",
        "\n",
        "# Print the head of each created DataFrame to inspect the parsed questionnaire responses\n",
        "print(\"\\n--- df_panas head ---\")\n",
        "print(df_panas.head())\n",
        "\n",
        "print(\"\\n--- df_stai head ---\")\n",
        "print(df_stai.head())\n",
        "\n",
        "print(\"\\n--- df_dim head ---\")\n",
        "print(df_dim.head())\n",
        "\n",
        "print(\"\\n--- df_sssq head ---\")\n",
        "print(df_sssq.head())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c847b950"
      },
      "source": [
        "## Create structured DataFrame\n",
        "\n",
        "### Subtask:\n",
        "Combine the extracted questionnaire responses into a single, comprehensive pandas DataFrame, ensuring each question's response is in its own column and rows represent individual entries or participants, as appropriate.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "135f5193"
      },
      "source": [
        "**Reasoning**:\n",
        "To combine the extracted questionnaire responses into a single DataFrame as instructed, I will first rename the columns of each individual questionnaire DataFrame with a descriptive prefix. Then, I will horizontally concatenate 'df_panas', 'df_stai', and 'df_dim', extend 'df_sssq' to match the row count, and finally concatenate it to the combined DataFrame, before printing the head and info of the resulting 'df_questionnaire_responses'.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a24052b9"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "# 1. Rename the columns of each DataFrame\n",
        "df_panas.columns = [f'PANAS_{i}' for i in range(len(df_panas.columns))]\n",
        "df_stai.columns = [f'STAI_{i}' for i in range(len(df_stai.columns))]\n",
        "df_dim.columns = [f'DIM_{i}' for i in range(len(df_dim.columns))]\n",
        "df_sssq.columns = [f'SSSQ_{i}' for i in range(len(df_sssq.columns))]\n",
        "\n",
        "# 2. Concatenate df_panas, df_stai, and df_dim horizontally\n",
        "df_combined_responses = pd.concat([df_panas, df_stai, df_dim], axis=1)\n",
        "\n",
        "# 3. Create a new DataFrame for df_sssq with the same number of rows as df_combined_responses\n",
        "# Get the number of rows from df_combined_responses\n",
        "num_rows = len(df_combined_responses)\n",
        "\n",
        "# Create an empty DataFrame with the correct number of rows and columns from df_sssq\n",
        "df_sssq_extended = pd.DataFrame(np.nan, index=range(num_rows), columns=df_sssq.columns)\n",
        "\n",
        "# Place the actual df_sssq data (which is a single row) into the first row of the extended DataFrame\n",
        "if not df_sssq.empty:\n",
        "    df_sssq_extended.iloc[0] = df_sssq.iloc[0]\n",
        "\n",
        "# 4. Horizontally concatenate the extended df_sssq DataFrame with df_combined_responses\n",
        "df_questionnaire_responses = pd.concat([df_combined_responses, df_sssq_extended], axis=1)\n",
        "\n",
        "# 5. Print the head and information of the final df_questionnaire_responses DataFrame\n",
        "print(\"--- df_questionnaire_responses head ---\")\n",
        "print(df_questionnaire_responses.head())\n",
        "\n",
        "print(\"\\n--- df_questionnaire_responses info ---\")\n",
        "df_questionnaire_responses.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0e0f466c"
      },
      "source": [
        "## Final Task\n",
        "\n",
        "### Subtask:\n",
        "Summarize the parsing process and the resulting structured DataFrame, including a preview of the cleaned questionnaire data and a brief discussion of its readiness for further analysis.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "49c2db2c"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "\n",
        "*   The initial `df_s2_quest` DataFrame was identified as a single-column structure where each row contained semicolon-separated values.\n",
        "*   The raw data contained distinct sections: header information (`#ORDER`, `#START`, `#END`), questionnaire responses (`#PANAS`, `#STAI`, `#DIM`, `#SSSQ`), and empty separator rows.\n",
        "*   Event timings were successfully extracted from the `#ORDER`, `#START`, and `#END` rows into a `df_event_timings` DataFrame. For instance, the 'Base' event has a Start\\_Time of 7.08 and an End\\_Time of 26.32.\n",
        "*   Individual questionnaire responses were parsed into separate DataFrames: `df_panas` (26 columns), `df_stai` (6 columns), `df_dim` (2 columns), and `df_sssq` (6 columns), handling varying response lengths with `NaN` values where necessary.\n",
        "*   The final combined `df_questionnaire_responses` DataFrame consists of 5 entries and 40 columns, with all data converted to `float64`. Columns were systematically renamed (e.g., `PANAS_0`, `STAI_0`), and `df_sssq` (originally a single row) was appropriately extended with `NaN` values to match the row count of other questionnaires during concatenation.\n",
        "\n",
        "### Insights or Next Steps\n",
        "\n",
        "*   The questionnaire data is now in a clean, structured format, making it ready for statistical analysis, such as calculating questionnaire scores, conducting correlation analysis with experimental events, or preparing for machine learning models.\n",
        "*   Further analysis could involve integrating `df_questionnaire_responses` with `df_event_timings` and other subject-specific data to build a holistic dataset for in-depth insights into experimental outcomes and participant responses.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "28b307a4"
      },
      "source": [
        "## Verify Saved Files\n",
        "\n",
        "### Subtask:\n",
        "List the contents of the `cleaned_data` directory to confirm that all DataFrames were successfully saved as CSV files."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d8bfd42e"
      },
      "source": [
        "**Reasoning**:\n",
        "To verify that the files have been saved, I will use the `!ls` command to list the contents of the `cleaned_data_dir`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "31e587a6"
      },
      "source": [
        "print(f\"Listing files in {cleaned_data_dir}:\")\n",
        "!ls -F {cleaned_data_dir}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2f8a94a4"
      },
      "source": [
        "# Task\n",
        "Upload the cleaned data files from the directory \"/content/WESAD_data/WESAD/S2/cleaned_data\" to a GitHub repository."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5d0affd2"
      },
      "source": [
        "## Configure Git and Add Files\n",
        "\n",
        "### Subtask:\n",
        "Initialize a Git repository if necessary, configure your Git user name and email, and then add the cleaned data files to the staging area. This might also involve navigating to the root of your repository.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e50a5893"
      },
      "source": [
        "**Reasoning**:\n",
        "I will navigate to the directory where the cleaned data files are saved, as this is the first step in initializing a Git repository and managing these files.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8713d937"
      },
      "source": [
        "import os\n",
        "\n",
        "# Ensure the directory exists before navigating\n",
        "os.makedirs(cleaned_data_dir, exist_ok=True)\n",
        "\n",
        "print(f\"Navigating to: {cleaned_data_dir}\")\n",
        "%cd {cleaned_data_dir}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3539613a"
      },
      "source": [
        "**Reasoning**:\n",
        "Now that the current working directory is set to `cleaned_data_dir`, I will initialize a Git repository, configure the Git user name and email, and add all files in the directory to the Git staging area, as instructed.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c47c6f1b"
      },
      "source": [
        "print('Initializing Git repository...')\n",
        "!git init\n",
        "\n",
        "print('Configuring Git user name and email...')\n",
        "!git config user.name \"Your Name\"  # Replace \"Your Name\" with your GitHub username\n",
        "!git config user.email \"your.email@example.com\"  # Replace \"your.email@example.com\" with your GitHub email\n",
        "\n",
        "print('Adding all files to the Git staging area...')\n",
        "!git add .\n",
        "\n",
        "print('Git initialization and staging complete.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c65148bb"
      },
      "source": [
        "**Reasoning**:\n",
        "Now that the files are in the staging area, I will commit them to the local Git repository with a descriptive message.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f4337609"
      },
      "source": [
        "print('Committing staged files...')\n",
        "!git commit -m \"Add cleaned WESAD S2 data files\"\n",
        "\n",
        "print('Commit complete.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4236159d"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous attempt to save cleaned data failed with a `NameError`, and consequently, the `git commit` command committed nothing as there were no files in the directory. Although the traceback suggested `df_event_timings` was not defined, the kernel state now shows both `df_event_timings` and `df_questionnaire_responses` as existing DataFrames. Therefore, I will re-attempt to save the cleaned `df_event_timings`, `df_questionnaire_responses`, and the sensor DataFrames to CSV files. This will ensure the files are actually present in the `cleaned_data_dir` before they are added to Git.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eef9998b"
      },
      "source": [
        "import os\n",
        "\n",
        "# Define the directory to save cleaned data\n",
        "cleaned_data_dir = f'{s2_data_path}/cleaned_data'\n",
        "\n",
        "# Create the directory if it does not exist\n",
        "os.makedirs(cleaned_data_dir, exist_ok=True)\n",
        "\n",
        "print(f\"Saving cleaned data to: {cleaned_data_dir}\")\n",
        "\n",
        "# Save df_event_timings\n",
        "df_event_timings.to_csv(f'{cleaned_data_dir}/df_event_timings.csv', index=False)\n",
        "print(\"Saved df_event_timings.csv\")\n",
        "\n",
        "# Save df_questionnaire_responses\n",
        "df_questionnaire_responses.to_csv(f'{cleaned_data_dir}/df_questionnaire_responses.csv', index=False)\n",
        "print(\"Saved df_questionnaire_responses.csv\")\n",
        "\n",
        "# Save individual chest sensor DataFrames\n",
        "for sensor_name, df in chest_dataframes.items():\n",
        "    df.to_csv(f'{cleaned_data_dir}/chest_{sensor_name.lower()}.csv', index=False)\n",
        "    print(f\"Saved chest_{sensor_name.lower()}.csv\")\n",
        "\n",
        "# Save individual wrist sensor DataFrames\n",
        "for sensor_name, df in wrist_dataframes.items():\n",
        "    df.to_csv(f'{cleaned_data_dir}/wrist_{sensor_name.lower()}.csv', index=False)\n",
        "    print(f\"Saved wrist_{sensor_name.lower()}.csv\")\n",
        "\n",
        "print(\"All cleaned DataFrames saved successfully!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e1ca9082"
      },
      "source": [
        "**Reasoning**:\n",
        "Since the files have now been successfully saved to the `cleaned_data_dir`, I will re-add all files in the current directory to the Git staging area to ensure the newly created CSVs are tracked. After staging, I will commit these files to the local Git repository.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ce9e358"
      },
      "source": [
        "print('Re-adding all files to the Git staging area...')\n",
        "!git add .\n",
        "\n",
        "print('Committing staged files...')\n",
        "!git commit -m \"Add cleaned WESAD S2 data files\"\n",
        "\n",
        "print('Commit complete.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "56023773"
      },
      "source": [
        "Before pushing to a GitHub repository, you'll need to create a new, empty repository on GitHub and get its URL. You will also need to authenticate with GitHub. The recommended way to do this in a Colab environment is using a Personal Access Token (PAT).\n",
        "\n",
        "**Instructions to generate a GitHub Personal Access Token (PAT):**\n",
        "1. Go to your GitHub profile settings.\n",
        "2. Navigate to `Developer settings` > `Personal access tokens` > `Tokens (classic)`.\n",
        "3. Click `Generate new token`.\n",
        "4. Give it a descriptive name (e.g., `Colab-Access`).\n",
        "5. Set an expiration date (e.g., 7 days or 30 days).\n",
        "6. Select the `repo` scope (or `public_repo` if you only plan to push to public repositories).\n",
        "7. Click `Generate token` and **copy the token immediately** (you won't be able to see it again).\n",
        "\n",
        "**Once you have your GitHub token and the new repository URL, execute the next code block.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b9cc4b74"
      },
      "source": [
        "**Reasoning**:\n",
        "Now that the files are committed locally and the user has been instructed on how to get a GitHub PAT, I will provide a code block that takes the user's PAT and repository URL as input, adds the remote origin, and then pushes the committed files to the specified GitHub repository.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d26b51a9"
      },
      "source": [
        "import getpass\n",
        "import os\n",
        "\n",
        "# 1. Prompt for credentials securely\n",
        "github_token = getpass.getpass('Enter your GitHub Personal Access Token: ')\n",
        "github_repo_url = input('Enter your GitHub repository URL (e.g., https://github.com/username/repo.git): ')\n",
        "\n",
        "# 2. Parse the URL to inject the token for authentication\n",
        "if \"https://github.com/\" in github_repo_url:\n",
        "    auth_repo_url = github_repo_url.replace(\n",
        "        \"https://github.com/\", f\"https://{github_token}@github.com/\"\n",
        "    )\n",
        "else:\n",
        "    auth_repo_url = github_repo_url\n",
        "\n",
        "# 3. Clean up existing remote and add the new authenticated one\n",
        "print('Configuring remote origin...')\n",
        "# Using '|| true' or a try-except style ensures this doesn't crash if origin doesn't exist\n",
        "!git remote remove origin 2>/dev/null || true\n",
        "!git remote add origin \"{auth_repo_url}\"\n",
        "\n",
        "# 4. Push to the repository\n",
        "print('Pushing to GitHub...')\n",
        "# Note: Newer GitHub repos use 'main' as the default branch instead of 'master'\n",
        "!git push -u origin master\n",
        "\n",
        "print('Push complete. Your files should now be on GitHub!')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import getpass\n",
        "\n",
        "# 1. Prompt for credentials securely\n",
        "github_token = getpass.getpass('Enter your GitHub Personal Access Token: ')\n",
        "github_repo_url = input('Enter your GitHub repository URL: ')\n",
        "\n",
        "# 2. Parse the URL to inject the token\n",
        "if \"https://github.com/\" in github_repo_url:\n",
        "    auth_repo_url = github_repo_url.replace(\n",
        "        \"https://github.com/\", f\"https://{github_token}@github.com/\"\n",
        "    )\n",
        "else:\n",
        "    auth_repo_url = github_repo_url\n",
        "\n",
        "print(\"Authentication URL prepared.\")"
      ],
      "metadata": {
        "id": "ujBzWR3ewDz_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Initialize Git LFS for large files\n",
        "print('Initializing Git LFS...')\n",
        "!git lfs install\n",
        "!git lfs track \"*.csv\"\n",
        "\n",
        "# 2. Clean up existing remote and add the authenticated one\n",
        "print('Configuring remote origin...')\n",
        "!git remote remove origin 2>/dev/null || true\n",
        "!git remote add origin \"{auth_repo_url}\"\n",
        "\n",
        "# 3. Verify the remote is set correctly\n",
        "!git remote -v"
      ],
      "metadata": {
        "id": "PHi7gU9KwJrt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Stage all files (including .gitattributes and your large CSV)\n",
        "print('Staging files...')\n",
        "!git add .\n",
        "\n",
        "# 2. Create the commit\n",
        "print('Committing changes...')\n",
        "!git commit -m \"Upload agentic AI project with LFS tracking for large datasets\"\n",
        "\n",
        "# 3. Push to the repository\n",
        "print('Pushing to GitHub (this may take a moment due to file size)...')\n",
        "!git push -u origin master\n",
        "\n",
        "print('Push complete! Your project is now live on GitHub.')"
      ],
      "metadata": {
        "id": "mK7sH4fVwN--"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Ensure all CSVs are actually handled by LFS\n",
        "!git lfs track \"*.csv\"\n",
        "!git add .gitattributes\n",
        "\n",
        "# 2. \"Undo\" the previous commit but keep your work (Reset to previous state)\n",
        "!git reset --soft HEAD~1\n",
        "\n",
        "# 3. Re-add everything. LFS will now intercept the large files properly.\n",
        "print(\"Re-staging files with LFS...\")\n",
        "!git add .\n",
        "\n",
        "# 4. Re-commit\n",
        "!git commit -m \"Corrected LFS tracking for agentic AI datasets\"\n",
        "\n",
        "# 5. Push\n",
        "print(\"Attempting push...\")\n",
        "!git push -u origin master"
      ],
      "metadata": {
        "id": "TmYX8E6lwhPK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. DELETE the corrupted history and start over\n",
        "!rm -rf .git\n",
        "\n",
        "# 2. Re-initialize Git\n",
        "!git init\n",
        "\n",
        "# 3. Setup LFS FIRST (before adding files)\n",
        "!git lfs install\n",
        "!git lfs track \"*.csv\"\n",
        "!git add .gitattributes\n",
        "\n",
        "# 4. Add your project files\n",
        "print(\"Staging files with LFS...\")\n",
        "!git add .\n",
        "!git commit -m \"Initial commit: Agentic AI project with proper LFS tracking\"\n",
        "\n",
        "# 5. Connect to the remote (using the auth_repo_url from your first cell)\n",
        "!git remote add origin \"{auth_repo_url}\"\n",
        "\n",
        "# 6. Force push to overwrite the broken history on GitHub\n",
        "print(\"Pushing clean history to GitHub...\")\n",
        "!git push -f -u origin master"
      ],
      "metadata": {
        "id": "oyYcdmw1wx2T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. DELETE the corrupted history and start over\n",
        "!rm -rf .git\n",
        "\n",
        "# 2. Re-initialize Git and set identity\n",
        "!git init\n",
        "!git config user.email \"michaeldomingo@sandiego.edu\"\n",
        "!git config user.name \"mdomingo2029\"\n",
        "\n",
        "# 3. Setup LFS FIRST (before adding files)\n",
        "print(\"Setting up Git LFS...\")\n",
        "!git lfs install\n",
        "!git lfs track \"*.csv\"\n",
        "!git add .gitattributes\n",
        "\n",
        "# 4. Add your project files\n",
        "print(\"Staging and committing files with LFS...\")\n",
        "!git add .\n",
        "!git commit -m \"Initial commit: Agentic AI project with proper LFS tracking\"\n",
        "\n",
        "# 5. Connect to the remote (Using the auth_repo_url from your previous setup)\n",
        "!git remote add origin \"{auth_repo_url}\"\n",
        "\n",
        "# 6. Force push to overwrite the broken history on GitHub\n",
        "print(\"Pushing clean history to GitHub... this may take a few minutes.\")\n",
        "!git push -f -u origin master"
      ],
      "metadata": {
        "id": "y3LLi0HHxBqB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Save the current state of the notebook file manually\n",
        "!git add data_cleaning.ipynb\n",
        "!git commit -m \"Manual save of data_cleaning notebook\"\n",
        "!git push origin master"
      ],
      "metadata": {
        "id": "t_3b2XGCzZ-G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Check exactly where you are and what files are visible\n",
        "print(\"Current Directory:\")\n",
        "!pwd\n",
        "print(\"\\nFiles in this folder:\")\n",
        "!ls\n",
        "\n",
        "# 2. Try to add the notebook using its full path or by moving back to /content\n",
        "# Replace 'data_cleaning.ipynb' with the actual name if it's different in the 'ls' list above.\n",
        "!git add /content/data_cleaning.ipynb\n",
        "!git commit -m \"Manual save of notebook from root\"\n",
        "!git push origin master"
      ],
      "metadata": {
        "id": "G53LN5MwzvFw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "Z0MJaHS7z7XD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Search for your notebook in Drive\n",
        "# Replace 'data_cleaning' with a keyword from your actual file name if different\n",
        "!find /content/drive/MyDrive -name \"*data_cleaning*.ipynb\"\n",
        "\n",
        "# 2. Once you see the path in the output, copy it here:\n",
        "# Example: !cp \"/content/drive/MyDrive/Colab Notebooks/data_cleaning.ipynb\" /content/\n",
        "# (Make sure to use quotes if there are spaces in the name)"
      ],
      "metadata": {
        "id": "5_NEm9vl1-PF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Copy the notebook from Drive to the active Git repository folder\n",
        "!cp \"/content/drive/MyDrive/Colab Notebooks/data_cleaning.ipynb\" /content/\n",
        "\n",
        "# 2. Add the notebook and any remaining data\n",
        "import os\n",
        "os.chdir('/content')\n",
        "!git add data_cleaning.ipynb\n",
        "!git add WESAD_data/\n",
        "\n",
        "# 3. Commit and Push\n",
        "print(\"Committing and pushing your final project...\")\n",
        "!git commit -m \"Final Project: Complete repository with notebook and LFS data\"\n",
        "!git push origin master\n",
        "\n",
        "print(\"\\nAll done! Your notebook and data are now live on GitHub.\")"
      ],
      "metadata": {
        "id": "FjZ59RtL2BqN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Increase the Git post buffer to handle large transfers (500MB)\n",
        "!git config --global http.postBuffer 524288000\n",
        "\n",
        "# 2. Ensure we are in the right folder\n",
        "import os\n",
        "os.chdir('/content')\n",
        "\n",
        "# 3. Attempt the push again with a focus on stability\n",
        "print(\"Resuming heavy upload (2.32 GB)... this will take several minutes.\")\n",
        "!git push -u origin master"
      ],
      "metadata": {
        "id": "LJe5DlrO_DX5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Ensure we are at the root\n",
        "import os\n",
        "os.chdir('/content')\n",
        "\n",
        "# 2. Increase buffer and stability settings\n",
        "!git config --global http.postBuffer 1048576000\n",
        "!git config --global ssh.postBuffer 1048576000\n",
        "!git config --global core.compression 0\n",
        "\n",
        "# 3. Push the 'WESAD_data' folder contents in small batches\n",
        "# First, let's push just the notebook and small files\n",
        "print(\"Pushing notebook and metadata...\")\n",
        "!git push origin master\n",
        "\n",
        "# 4. If that succeeded, push the large LFS objects\n",
        "# Note: LFS usually handles its own transfers, but a manual push helps resume\n",
        "print(\"Resuming LFS data transfer...\")\n",
        "!git lfs push origin master"
      ],
      "metadata": {
        "id": "UV1PuUI9HsdV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "    import numpy as np\n",
        "    import scipy.signal\n",
        "    import pandas as pd\n",
        "\n",
        "    def resample_sensor_data(df, target_freq, current_freq):\n",
        "        if 'timestamp' not in df.columns:\n",
        "            print(\"Timestamp column missing, cannot resample based on time. Assuming uniform sampling.\")\n",
        "            num_samples = len(df)\n",
        "            time_duration = num_samples / current_freq\n",
        "            new_num_samples = int(time_duration * target_freq)\n",
        "\n",
        "            resampled_df = pd.DataFrame()\n",
        "            for col in df.columns:\n",
        "                if np.issubdtype(df[col].dtype, np.number): # Only resample numeric columns\n",
        "                    resampled_data = scipy.signal.resample(df[col].values, new_num_samples)\n",
        "                    resampled_df[col] = resampled_data\n",
        "                else:\n",
        "                    # For non-numeric, we can't directly resample, maybe forward fill or skip\n",
        "                    # For now, let's just carry over if index matches, though index won't align\n",
        "                    pass\n",
        "            # Need to create a new time index for resampled_df\n",
        "            new_time_index = np.linspace(0, time_duration, new_num_samples, endpoint=False)\n",
        "            # If we had an original start time, we'd add it here.\n",
        "            # resampled_df['timestamp'] = new_time_index + (original_start_time if available)\n",
        "            # Since we don't have original timestamp, we create a relative one.\n",
        "            resampled_df.insert(0, 'relative_time', new_time_index)\n",
        "\n",
        "        else:\n",
        "            time_seconds = (df['timestamp'] - df['timestamp'].iloc[0]) / np.timedelta64(1, 's')\n",
        "            time_duration = time_seconds.iloc[-1]\n",
        "            new_num_samples = int(time_duration * target_freq)\n",
        "            new_time_index = np.linspace(0, time_duration, new_num_samples, endpoint=False)\n",
        "\n",
        "            resampled_df = pd.DataFrame()\n",
        "            resampled_df['timestamp_new'] = pd.to_timedelta(new_time_index, unit='s') + df['timestamp'].iloc[0]\n",
        "\n",
        "            for col in df.columns:\n",
        "                if col != 'timestamp' and np.issubdtype(df[col].dtype, np.number):\n",
        "                    resampled_data = np.interp(new_time_index, time_seconds.values, df[col].values)\n",
        "                    resampled_df[col] = resampled_data\n",
        "            resampled_df = resampled_df.rename(columns={'timestamp_new': 'timestamp'})\n",
        "\n",
        "        return resampled_df\n",
        "\n",
        "    # Assuming chest_dataframes and wrist_dataframes are already loaded\n",
        "    # and contain DataFrames for each sensor, with original frequencies known.\n",
        "\n",
        "    # Example frequencies (replace with actual frequencies if known and different)\n",
        "    chest_freq = 700\n",
        "    wrist_freqs = {'ACC': 32, 'BVP': 64, 'EDA': 4, 'TEMP': 4} # Example freqs for wrist sensors\n",
        "\n",
        "    target_freq = 64\n",
        "\n",
        "    print(\"Resampling chest dataframes...\")\n",
        "    for sensor_name, df in chest_dataframes.items():\n",
        "        print(f\"Resampling {sensor_name} from {chest_freq}Hz to {target_freq}Hz\")\n",
        "        chest_dataframes[sensor_name] = resample_sensor_data(df.copy(), target_freq, chest_freq)\n",
        "        print(f\"New shape of {sensor_name}: {chest_dataframes[sensor_name].shape}\")\n",
        "\n",
        "    print(\"\\nResampling wrist dataframes...\")\n",
        "    for sensor_name, df in wrist_dataframes.items():\n",
        "        current_f = wrist_freqs.get(sensor_name, 32) # Default to 32 if not in map\n",
        "        print(f\"Resampling {sensor_name} from {current_f}Hz to {target_freq}Hz\")\n",
        "        wrist_dataframes[sensor_name] = resample_sensor_data(df.copy(), target_freq, current_f)\n",
        "        print(f\"New shape of {sensor_name}: {wrist_dataframes[sensor_name].shape}\")\n",
        "\n",
        "    print(\"\\nResampling complete.\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qMecDoMiFdXG",
        "outputId": "da664761-43ee-4f06-9a55-6e84b0ddfb43"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Resampling chest dataframes...\n",
            "\n",
            "Resampling wrist dataframes...\n",
            "\n",
            "Resampling complete.\n"
          ]
        }
      ]
    }
  ]
}